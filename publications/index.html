<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>publications | Ruohan Gao</title> <meta name="author" content="Ruohan Gao"> <meta name="description" content="publications by categories in reversed chronological order. generated by jekyll-scholar."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ruohangao.github.io/publications/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Ruohan </span>Gao</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/projects/">projects</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description">publications by categories in reversed chronological order. generated by jekyll-scholar.</p> </header> <article> <div class="publications"> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/of_benchmark_cvpr2023-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/of_benchmark_cvpr2023-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/of_benchmark_cvpr2023-1400.webp"></source> <img src="/assets/img/publication_preview/of_benchmark_cvpr2023.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="of_benchmark_cvpr2023.jpg" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gao2023ObjectFolderBM" class="col-sm-8"> <div class="title">The ObjectFolder Benchmark: Multisensory Object-Centric Learning with Neural and Real Objects</div> <div class="author"> <em>Ruohan Gao*</em>, Yiming Dou*, Hao Li*, Tanmay Agarwal, Jeannette Bohg, Yunzhu Li, Li Fei-Fei, and Jiajun Wu</div> <div class="periodical"> <em>In Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/realimpact_cvpr2023.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/objectfolder" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://objectfolder.stanford.edu/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2023ObjectFolderBM</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{The ObjectFolder Benchmark: Multisensory Object-Centric Learning with Neural and Real Objects}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao*, Ruohan and Dou*, Yiming and Li*, Hao and Agarwal, Tanmay and Bohg, Jeannette and Li, Yunzhu and Fei-Fei, Li and Wu, Jiajun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">demo</span> <span class="p">=</span> <span class="s">{https://www.objectfolder.org/swan_vis/}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/realimpact_cvpr2023-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/realimpact_cvpr2023-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/realimpact_cvpr2023-1400.webp"></source> <img src="/assets/img/publication_preview/realimpact_cvpr2023.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="realimpact_cvpr2023.jpg" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="clarke2023realimpact" class="col-sm-8"> <div class="title">RealImpact: A Dataset of Impact Sound Fields for Real Objects</div> <div class="author"> Samuel Clarke, <em>Ruohan Gao</em>, Mason Wang, Mark Rau, Julia Xu, Mark Rau, Jui-Hsien Wang, Doug James, and Jiajun Wu</div> <div class="periodical"> <em>In Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2306.00956.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/pdf/RealImpact_supp" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://github.com/samuel-clarke/RealImpact" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="hhttps://samuelpclarke.com/realimpact/" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">clarke2023realimpact</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{RealImpact: A Dataset of Impact Sound Fields for Real Objects}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Clarke, Samuel and Gao, Ruohan and Wang, Mason and Rau, Mark and Xu, Julia and Rau, Mark and Wang, Jui-Hsien and James, Doug and Wu, Jiajun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/osf_tmlr2023-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/osf_tmlr2023-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/osf_tmlr2023-1400.webp"></source> <img src="/assets/img/publication_preview/osf_tmlr2023.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="osf_tmlr2023.jpg" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="yu2023osf" class="col-sm-8"> <div class="title">Learning Object-Centric Neural Scattering Functions for Free-Viewpoint Relighting and Scene Composition</div> <div class="author"> Hong-Xing Yu*, Michelle Guo*, Alireza Fathi, Yen-Yu Chang, Eric Ryan Chan, <em>Ruohan Gao</em>, Thomas Funkhouser, and Jiajun Wu</div> <div class="periodical"> <em></em> 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2303.06138.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/pdf/2023_TMLR_OSF_supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://github.com/michguo/osf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://kovenyu.com/osf/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">yu2023osf</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning Object-Centric Neural Scattering Functions for Free-Viewpoint Relighting and Scene Composition}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yu*, Hong-Xing and Guo*, Michelle and Fathi, Alireza and Chang, Yen-Yu and Chan, Eric Ryan and Gao, Ruohan and Funkhouser, Thomas and Wu, Jiajun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Transactions on Machine Learning Research (TMLR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/dano_ral2023-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/dano_ral2023-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/dano_ral2023-1400.webp"></source> <img src="/assets/img/publication_preview/dano_ral2023.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="dano_ral2023.jpg" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="simon2023ral" class="col-sm-8"> <div class="title">Differentiable Physics Simulation of Dynamics-Augmented Neural Objects</div> <div class="author"> Simon Le Cleac’h, Hong-Xing Yu, Michelle Guo, Taylor A. Howell, <em>Ruohan Gao</em>, Jiajun Wu, Zachary Manchester, and Mac Schwager</div> <div class="periodical"> <em>Robotics and Automation Letters (RA-L)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2210.09420.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">simon2023ral</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Differentiable Physics Simulation of Dynamics-Augmented Neural Objects}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cleac'h, Simon Le and Yu, Hong-Xing and Guo, Michelle and Howell, Taylor A. and Gao, Ruohan and Wu, Jiajun and Manchester, Zachary and Schwager, Mac}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Robotics and Automation Letters (RA-L)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/sonicverse_icra2023-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/sonicverse_icra2023-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/sonicverse_icra2023-1400.webp"></source> <img src="/assets/img/publication_preview/sonicverse_icra2023.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="sonicverse_icra2023.jpg" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gao2023sonicverse" class="col-sm-8"> <div class="title">Sonicverse: A Multisensory Simulation Platform for Training Household Agents that See and Hear</div> <div class="author"> <em>Ruohan Gao*</em>, Hao Li*, Gokul Dharan, Zhuzhu Wang, Chengshu Li, Fei Xia, Silvio Savarese, Li Fei-Fei, and Jiajun Wu</div> <div class="periodical"> <em>In International Conference on Robotics and Automation (ICRA),</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/gao-icra2023-sonicverse.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/StanfordVL/sonicverse" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://ai.stanford.edu/%C2%A0rhgao/sonicverse/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2023sonicverse</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Sonicverse: A Multisensory Simulation Platform for Training Household Agents that See and Hear}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao*, Ruohan and Li*, Hao and Dharan, Gokul and Wang, Zhuzhu and Li, Chengshu and Xia, Fei and Savarese, Silvio and Fei-Fei, Li and Wu, Jiajun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Robotics and Automation (ICRA),}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/emma_dataset_iclr2023-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/emma_dataset_iclr2023-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/emma_dataset_iclr2023-1400.webp"></source> <img src="/assets/img/publication_preview/emma_dataset_iclr2023.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="emma_dataset_iclr2023.jpg" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="standley2023emma" class="col-sm-8"> <div class="title">An Extensible Multi-modal Multi-task Object Dataset with Materials</div> <div class="author"> Trevor Scott Standley, <em>Ruohan Gao</em>, Dawn Chen, Jiajun Wu, and Silvio Savarese</div> <div class="periodical"> <em>In International Conference on Learning Representations (ICLR)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/emma_iclr2023.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://emma.stanford.edu/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">standley2023emma</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{An Extensible Multi-modal Multi-task Object Dataset with Materials}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Standley, Trevor Scott and Gao, Ruohan and Chen, Dawn and Wu, Jiajun and Savarese, Silvio}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Learning Representations (ICLR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">data</span> <span class="p">=</span> <span class="s">{https://docs.google.com/forms/d/e/1FAIpQLScOO2YEeuOPyCusWx6KKS1kt8hGe6CCn2mtxr17yimTLe96qw/viewform}</span><span class="p">,</span>
  <span class="na">demo</span> <span class="p">=</span> <span class="s">{https://emma-app.stanford.edu/}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/see_hear_feel_corl2022-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/see_hear_feel_corl2022-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/see_hear_feel_corl2022-1400.webp"></source> <img src="/assets/img/publication_preview/see_hear_feel_corl2022.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="see_hear_feel_corl2022.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="li2022seehearfeel" class="col-sm-8"> <div class="title">See, Hear, and Feel: Smart Sensory Fusion for Robotic Manipulation</div> <div class="author"> Hao Li*, Yizhi Zhang*, Junzhe Zhu, Shaoxiong Wang, Michelle A. Lee, Huazhe Xu, Edward Adelson, Li Fei-Fei, <em>Ruohan Gao†</em>, and Jiajun Wu†</div> <div class="periodical"> <em>In Conference on Robot Learning (CoRL)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2212.03858.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/pdf/seel_hear_feel_supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://ai.stanford.edu/%C2%A0rhgao/see_hear_feel/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">li2022seehearfeel</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{See, Hear, and Feel: Smart Sensory Fusion for Robotic Manipulation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li*, Hao and Zhang*, Yizhi and Zhu, Junzhe and Wang, Shaoxiong and Lee, Michelle A. and Xu, Huazhe and Adelson, Edward and Fei-Fei, Li and Gao†, Ruohan and Wu†, Jiajun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Robot Learning (CoRL)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/objectfolderV2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/objectfolderV2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/objectfolderV2-1400.webp"></source> <img src="/assets/img/publication_preview/objectfolderV2.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="objectfolderV2.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gao2022ObjectFolderV2" class="col-sm-8"> <div class="title">ObjectFolder 2.0: A Multisensory Object Dataset for Sim2Real Transfer</div> <div class="author"> <em>Ruohan Gao*</em>, Zilin Si*, Yen-Yu Chang*, Samuel Clarke, Jeannette Bohg, Li Fei-Fei, Wenzhen Yuan, and Jiajun Wu</div> <div class="periodical"> <em>In Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2204.02389.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/pdf/ObjectFolderV2_Supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://ai.stanford.edu/%C2%A0rhgao/objectfolder2.0/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2022ObjectFolderV2</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ObjectFolder 2.0: A Multisensory Object Dataset for Sim2Real Transfer}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao*, Ruohan and Si*, Zilin and Chang*, Yen-Yu and Clarke, Samuel and Bohg, Jeannette and Fei-Fei, Li and Yuan, Wenzhen and Wu, Jiajun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">data</span> <span class="p">=</span> <span class="s">{https://github.com/rhgao/ObjectFolder}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/visual_acoustic_matching_cvpr2022-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/visual_acoustic_matching_cvpr2022-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/visual_acoustic_matching_cvpr2022-1400.webp"></source> <img src="/assets/img/publication_preview/visual_acoustic_matching_cvpr2022.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="visual_acoustic_matching_cvpr2022.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="chen2022visual" class="col-sm-8"> <div class="title">Visual Acoustic Matching</div> <div class="author"> Changan Chen, <em>Ruohan Gao</em>, Paul Calamia, and Kristen Grauman</div> <div class="periodical"> <em>In Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2202.06875.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/facebookresearch/visual-acoustic-matching" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://vision.cs.utexas.edu/projects/visual-acoustic-matching/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">chen2022visual</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Visual Acoustic Matching}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chen, Changan and Gao, Ruohan and Calamia, Paul and Grauman, Kristen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/objectfolder_corl2021-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/objectfolder_corl2021-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/objectfolder_corl2021-1400.webp"></source> <img src="/assets/img/publication_preview/objectfolder_corl2021.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="objectfolder_corl2021.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gao2021ObjectFolder" class="col-sm-8"> <div class="title">ObjectFolder: A Dataset of Objects with Implicit Visual, Auditory, and Tactile Representations</div> <div class="author"> <em>Ruohan Gao</em>, Yen-Yu Chang, Shivani Mall, Li Fei-Fei, and Jiajun Wu</div> <div class="periodical"> <em>In Conference on Robot Learning (CoRL)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2109.07991.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/pdf/ObjectFolder_Supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://ai.stanford.edu/%C2%A0rhgao/objectfolder/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2021ObjectFolder</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ObjectFolder: A Dataset of Objects with Implicit Visual, Auditory, and Tactile Representations}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao, Ruohan and Chang, Yen-Yu and Mall, Shivani and Fei-Fei, Li and Wu, Jiajun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Robot Learning (CoRL)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">data</span> <span class="p">=</span> <span class="s">{https://github.com/rhgao/ObjectFolder}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/diffImpact_corl2021-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/diffImpact_corl2021-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/diffImpact_corl2021-1400.webp"></source> <img src="/assets/img/publication_preview/diffImpact_corl2021.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="diffImpact_corl2021.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="clarke2021diffimpact" class="col-sm-8"> <div class="title">DiffImpact: Differentiable Rendering and Identification of Impact Sounds</div> <div class="author"> Samuel Clarke, Negin Heravi, Mark Rau, <em>Ruohan Gao</em>, Jiajun Wu, Doug James, and Jeannette Bohg</div> <div class="periodical"> <em>In Conference on Robot Learning (CoRL)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/clarke-corl2021.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/diffImpact_supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://github.com/samuel-clarke/diffimpact" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://sites.google.com/view/diffimpact" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">clarke2021diffimpact</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DiffImpact: Differentiable Rendering and Identification of Impact Sounds}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Clarke, Samuel and Heravi, Negin and Rau, Mark and Gao, Ruohan and Wu, Jiajun and James, Doug and Bohg, Jeannette}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Robot Learning (CoRL)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/bmvc2021-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/bmvc2021-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/bmvc2021-1400.webp"></source> <img src="/assets/img/publication_preview/bmvc2021.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="bmvc2021.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="garg2021geometry" class="col-sm-8"> <div class="title">Geometry-Aware Multi-Task Learning for Binaural Audio Generation from Video</div> <div class="author"> Rishabh Garg, <em>Ruohan Gao</em>, and Kristen Grauman</div> <div class="periodical"> <em>In British Machine Vision Conference (BMVC)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2111.10882.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/pdf/bmvc2021_supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://vision.cs.utexas.edu/projects/geometry-aware-binaural/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">garg2021geometry</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Geometry-Aware Multi-Task Learning for Binaural Audio Generation from Video}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Garg, Rishabh and Gao, Ruohan and Grauman, Kristen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{British Machine Vision Conference (BMVC)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">data</span> <span class="p">=</span> <span class="s">{https://vision.cs.utexas.edu/projects/geometry-aware-binaural/#dataset}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/thesis_teaser-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/thesis_teaser-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/thesis_teaser-1400.webp"></source> <img src="/assets/img/publication_preview/thesis_teaser.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="thesis_teaser.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gao2021dissertation" class="col-sm-8"> <div class="title">Look and Listen: From Semantic to Spatial Audio-Visual Perception</div> <div class="author"> <em>Ruohan Gao</em> </div> <div class="periodical"> <em>In Ph.D. Dissertation</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Ruohan_Gao_dissertation.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2021dissertation</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Look and Listen: From Semantic to Spatial Audio-Visual Perception}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao, Ruohan}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Ph.D. Dissertation}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">media</span> <span class="p">=</span> <span class="s">{https://gradschool.utexas.edu/news/graduate-school-announces-2021-award-winners}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/VisualVoice_cvpr2021-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/VisualVoice_cvpr2021-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/VisualVoice_cvpr2021-1400.webp"></source> <img src="/assets/img/publication_preview/VisualVoice_cvpr2021.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="VisualVoice_cvpr2021.jpg" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gao2021visualvoice" class="col-sm-8"> <div class="title">Visualvoice: Audio-visual speech separation with cross-modal consistency</div> <div class="author"> <em>Ruohan Gao</em>, and Kristen Grauman</div> <div class="periodical"> <em>In Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/gao2021VisualVoice.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/VisualVoice_Supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://github.com/facebookresearch/VisualVoice" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://vision.cs.utexas.edu/projects/VisualVoice/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2021visualvoice</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Visualvoice: Audio-visual speech separation with cross-modal consistency}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao, Ruohan and Grauman, Kristen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/av_wan_iclr2021-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/av_wan_iclr2021-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/av_wan_iclr2021-1400.webp"></source> <img src="/assets/img/publication_preview/av_wan_iclr2021.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="av_wan_iclr2021.jpg" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="chen2021waypoints" class="col-sm-8"> <div class="title">Learning to Set Waypoints for Audio-Visual Navigation</div> <div class="author"> Changan Chen, Sagnik Majumder, Ziad Al-Halah, <em>Ruohan Gao</em>, Santhosh Kumar Ramakrishnan, and Kristen Grauman</div> <div class="periodical"> <em>In International Conference on Learning Representations (ICLR)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2008.09622.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/facebookresearch/sound-spaces/tree/main/ss_baselines/av_wan" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://vision.cs.utexas.edu/projects/audio_visual_waypoints/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">chen2021waypoints</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning to Set Waypoints for Audio-Visual Navigation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chen, Changan and Majumder, Sagnik and Al-Halah, Ziad and Gao, Ruohan and Ramakrishnan, Santhosh Kumar and Grauman, Kristen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Learning Representations (ICLR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/visualEchoes_eccv2020-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/visualEchoes_eccv2020-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/visualEchoes_eccv2020-1400.webp"></source> <img src="/assets/img/publication_preview/visualEchoes_eccv2020.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="visualEchoes_eccv2020.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gao2020visualechoes" class="col-sm-8"> <div class="title">VisualEchoes: Spatial Visual Representation Learning through Echolocation</div> <div class="author"> <em>Ruohan Gao</em>, Changan Chen, Ziad Al-Halah, Carl Schissler, and Kristen Grauman</div> <div class="periodical"> <em>In European Conference on Computer Vision (ECCV)</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/gao-eccv2020-visualechoes.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/visualechoes_supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://vision.cs.utexas.edu/projects/visualEchoes/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2020visualechoes</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{VisualEchoes: Spatial Visual Representation Learning through Echolocation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao, Ruohan and Chen, Changan and Al-Halah, Ziad and Schissler, Carl and Grauman, Kristen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{European Conference on Computer Vision (ECCV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">data</span> <span class="p">=</span> <span class="s">{https://github.com/facebookresearch/VisualEchoes}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/listen_to_look_cvpr2020-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/listen_to_look_cvpr2020-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/listen_to_look_cvpr2020-1400.webp"></source> <img src="/assets/img/publication_preview/listen_to_look_cvpr2020.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="listen_to_look_cvpr2020.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gao2020listentolook" class="col-sm-8"> <div class="title">Listen to Look: Action Recognition by Previewing Audio</div> <div class="author"> <em>Ruohan Gao</em>, Tae-Hyun Oh, Kristen Grauman, and Lorenzo Torresani</div> <div class="periodical"> <em>In Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/gao-cvpr2020-listen-to-look.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/listen-to-look-cvpr2020-supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://github.com/facebookresearch/Listen-to-Look" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/listen-to-look-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://vision.cs.utexas.edu/projects/listen_to_look/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2020listentolook</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Listen to Look: Action Recognition by Previewing Audio}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao, Ruohan and Oh, Tae-Hyun and Grauman, Kristen and Torresani, Lorenzo}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/co-separation-iccv2019-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/co-separation-iccv2019-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/co-separation-iccv2019-1400.webp"></source> <img src="/assets/img/publication_preview/co-separation-iccv2019.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="co-separation-iccv2019.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gao2019coseparation" class="col-sm-8"> <div class="title">Co-Separating Sounds of Visual Objects</div> <div class="author"> <em>Ruohan Gao</em>, and Kristen Grauman</div> <div class="periodical"> <em>In International Conference on Computer Vision (ICCV)</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/coseparation-iccv2019.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/coseparation-iccv2019-supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://github.com/rhgao/co-separation" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/coseparation-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://vision.cs.utexas.edu/projects/coseparation/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2019coseparation</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Co-Separating Sounds of Visual Objects}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao, Ruohan and Grauman, Kristen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Computer Vision (ICCV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/2.5D_visual_sound_cvpr2019-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/2.5D_visual_sound_cvpr2019-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/2.5D_visual_sound_cvpr2019-1400.webp"></source> <img src="/assets/img/publication_preview/2.5D_visual_sound_cvpr2019.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="2.5D_visual_sound_cvpr2019.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gao2019visual-sound" class="col-sm-8"> <div class="title">2.5D Visual Sound</div> <div class="author"> <em>Ruohan Gao</em>, and Kristen Grauman</div> <div class="periodical"> <em>In Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/gao-cvpr2019.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/gao-cvpr2019-supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://github.com/facebookresearch/2.5D-Visual-Sound" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://vision.cs.utexas.edu/projects/2.5D_visual_sound/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2019visual-sound</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{2.5D Visual Sound}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao, Ruohan and Grauman, Kristen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">data</span> <span class="p">=</span> <span class="s">{https://github.com/facebookresearch/FAIR-Play}</span><span class="p">,</span>
  <span class="na">media</span> <span class="p">=</span> <span class="s">{https://www.technologyreview.com/2018/12/26/138088/deep-learning-turns-mono-recordings-into-immersive-sound/}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/audioobjects_eccv2018-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/audioobjects_eccv2018-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/audioobjects_eccv2018-1400.webp"></source> <img src="/assets/img/publication_preview/audioobjects_eccv2018.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="audioobjects_eccv2018.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gao2018object-sounds" class="col-sm-8"> <div class="title">Learning to Separate Object Sounds by Watching Unlabeled Video</div> <div class="author"> <em>Ruohan Gao</em>, Rogerio Feris, and Kristen Grauman</div> <div class="periodical"> <em>In European Conference on Computer Vision (ECCV)</em>, 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/sound-sep-eccv2018.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/sound-sep-eccv2018_supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://github.com/rhgao/Deep-MIML-Network" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/gao-eccv2018-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://vision.cs.utexas.edu/projects/separating_object_sounds/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2018object-sounds</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning to Separate Object Sounds by Watching Unlabeled Video}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao, Ruohan and Feris, Rogerio and Grauman, Kristen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{European Conference on Computer Vision (ECCV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/shapecodes_eccv2018-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/shapecodes_eccv2018-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/shapecodes_eccv2018-1400.webp"></source> <img src="/assets/img/publication_preview/shapecodes_eccv2018.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="shapecodes_eccv2018.jpg" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="jayaraman2018shape" class="col-sm-8"> <div class="title">ShapeCodes: Self-Supervised Feature Learning by Lifting Views to Viewgrids</div> <div class="author"> Dinesh Jayaraman, <em>Ruohan Gao</em>, and Kristen Grauman</div> <div class="periodical"> <em>In European Conference on Computer Vision (ECCV)</em>, 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/shape-codes-eccv2018.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/shape-codes-eccv2018-supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">jayaraman2018shape</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ShapeCodes: Self-Supervised Feature Learning by Lifting Views to Viewgrids}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jayaraman, Dinesh and Gao, Ruohan and Grauman, Kristen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{European Conference on Computer Vision (ECCV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/im2flow_cvpr2018-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/im2flow_cvpr2018-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/im2flow_cvpr2018-1400.webp"></source> <img src="/assets/img/publication_preview/im2flow_cvpr2018.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="im2flow_cvpr2018.jpg" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gao2018im2flow" class="col-sm-8"> <div class="title">Im2Flow: Motion Hallucination from Static Images for Action Recognition</div> <div class="author"> <em>Ruohan Gao</em>, Bo Xiong, and Kristen Grauman</div> <div class="periodical"> <em>In Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/gao-cvpr2018.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/gao-cvpr2018-supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://github.com/rhgao/Im2Flow" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/gao-cvpr2018-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://vision.cs.utexas.edu/projects/im2flow/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2018im2flow</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Im2Flow: Motion Hallucination from Static Images for Action Recognition}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao, Ruohan and Xiong, Bo and Grauman, Kristen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/ondemand_iccv2017-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/ondemand_iccv2017-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/ondemand_iccv2017-1400.webp"></source> <img src="/assets/img/publication_preview/ondemand_iccv2017.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="ondemand_iccv2017.jpg" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gao2017on-demand" class="col-sm-8"> <div class="title">On-Demand Learning for Deep Image Restoration</div> <div class="author"> <em>Ruohan Gao</em>, and Kristen Grauman</div> <div class="periodical"> <em>In International Conference on Computer Vision (ICCV)</em>, 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/gao-iccv2017.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/gao-iccv2017-supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://github.com/rhgao/on-demand-learning/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/gao-iccv2017-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://vision.cs.utexas.edu/projects/on_demand_learning/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2017on-demand</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{On-Demand Learning for Deep Image Restoration}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao, Ruohan and Grauman, Kristen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Computer Vision (ICCV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2016</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/objectcentric_accv2016-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/objectcentric_accv2016-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/objectcentric_accv2016-1400.webp"></source> <img src="/assets/img/publication_preview/objectcentric_accv2016.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="objectcentric_accv2016.jpg" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gao2016object-centric" class="col-sm-8"> <div class="title">Object-Centric Representation Learning from Unlabeled Videos</div> <div class="author"> <em>Ruohan Gao</em>, Dinesh Jayaraman, and Kristen Grauman</div> <div class="periodical"> <em>In Asian Conference on Computer Vision (ACCV)</em>, 2016 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/gao-accv2016.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/gao-accv2016-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://vision.cs.utexas.edu/projects/object_centric_unsup/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2016object-centric</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Object-Centric Representation Learning from Unlabeled Videos}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao, Ruohan and Jayaraman, Dinesh and Grauman, Kristen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Asian Conference on Computer Vision (ACCV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">IEEE ICC</abbr></div> <div id="gao2016accelerating" class="col-sm-8"> <div class="title">Accelerating Graph Mining Algorithms via Uniform Random Edge Sampling</div> <div class="author"> <em>Ruohan Gao</em>, Huanle Xu, Pili Hu, and Wing Cheong Lau</div> <div class="periodical"> <em>In IEEE International Conference on Communications (ICC)</em>, 2016 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/graphsampling_icc2016.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2016accelerating</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Accelerating Graph Mining Algorithms via Uniform Random Edge Sampling}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao, Ruohan and Xu, Huanle and Hu, Pili and and Lau, Wing Cheong}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE International Conference on Communications (ICC)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2015</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">IEEE GLOBECOM</abbr></div> <div id="gao2015graph" class="col-sm-8"> <div class="title">Graph Property Preservation under Community-Based Sampling</div> <div class="author"> <em>Ruohan Gao</em>, Pili Hu, and Wing Cheong Lau</div> <div class="periodical"> <em>In IEEE Global Communications Conference (GLOBECOM)</em>, 2015 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/CBS_globecom2015.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2015graph</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Graph Property Preservation under Community-Based Sampling}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao, Ruohan and Hu, Pili and and Lau, Wing Cheong}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE Global Communications Conference (GLOBECOM)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2015}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Ruohan Gao. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>
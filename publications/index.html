<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Publications | Ruohan Gao</title> <meta name="author" content="Ruohan Gao"> <meta name="description" content="*equal contribution, †equal advising "> <meta name="keywords" content="Ruohan Gao, Homepage"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ruohangao.github.io/publications/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Ruohan </span>Gao</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Home</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching</a> </li> <li class="nav-item"> <a class="nav-link" href="/assets/Ruohan_Gao_CV.pdf">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description">*equal contribution, †equal advising </p> </header> <article> <div class="publications"> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/soundcam_neurips2023-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/soundcam_neurips2023-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/soundcam_neurips2023-1400.webp"></source> <img src="/assets/img/publication_preview/soundcam_neurips2023.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="soundcam_neurips2023.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2023soundcam" class="col-sm-8"> <div class="title">SoundCam: A Dataset for Tasks in Tracking and Identifying Humans from Real Room Acoustics</div> <div class="author"> Mason Wang*, <a href="https://samuelpclarke.com/" rel="external nofollow noopener" target="_blank">Samuel Clarke*</a>, <a href="http://juiwang.com/" rel="external nofollow noopener" target="_blank">Jui-Hsien Wang</a>, <em>Ruohan Gao</em>, and <a href="https://jiajunwu.com/" rel="external nofollow noopener" target="_blank">Jiajun Wu</a> </div> <div class="periodical"> <em>Conference on Neural Information Processing Systems Datasets and Benchmarks Track (NeurIPS)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openreview.net/pdf?id=oQSfcVTNr1" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://purl.stanford.edu/xq364hd5023" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Dataset</a> <a href="https://www.youtube.com/watch?v=HAhJLgj8maI" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://sites.google.com/view/soundcam" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wang2023soundcam</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SoundCam: A Dataset for Tasks in Tracking and Identifying Humans from Real Room Acoustics}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang*, Mason and Clarke*, Samuel and Wang, Jui-Hsien and Gao, Ruohan and Wu, Jiajun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Neural Information Processing Systems Datasets and Benchmarks Track (NeurIPS)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/noir_corl2023-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/noir_corl2023-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/noir_corl2023-1400.webp"></source> <img src="/assets/img/publication_preview/noir_corl2023.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="noir_corl2023.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Lee2023noir" class="col-sm-8"> <div class="title">NOIR: Neural Signal Operated Intelligent Robot for Everyday Activities</div> <div class="author"> Sharon Lee*, <a href="https://ai.stanford.edu/~zharu/" rel="external nofollow noopener" target="_blank">Ruohan Zhang*</a>, <a href="https://mj-hwang.github.io/" rel="external nofollow noopener" target="_blank">Minjune Hwang*</a>, Ayano Hiranaka*, <a href="https://www.chenwangjeremy.net/" rel="external nofollow noopener" target="_blank">Chen Wang</a>, Wensi Ai, Jin Jie Ryan Tan, Shreya Gupta, Yilun Hao, Gabrael Levine, <em>Ruohan Gao</em>, <a href="https://profiles.stanford.edu/anthony-norcia/" rel="external nofollow noopener" target="_blank">Anthony Norcia</a>, <a href="https://jiajunwu.com/" rel="external nofollow noopener" target="_blank">Jiajun Wu†</a>, and <a href="https://profiles.stanford.edu/fei-fei-li" rel="external nofollow noopener" target="_blank">Li Fei-Fei†</a> </div> <div class="periodical"> <em>Conference on Robot Learning (CoRL)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openreview.net/pdf?id=eyykI3UIHa" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://sites.google.com/view/noir-corl2023/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Lee2023noir</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{NOIR: Neural Signal Operated Intelligent Robot for Everyday Activities}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lee*, Sharon and Zhang*, Ruohan and Hwang*, Minjune and Hiranaka*, Ayano and Wang, Chen and Ai, Wensi and Tan, Jin Jie Ryan and Gupta, Shreya and Hao, Yilun and Levine, Gabrael and Gao, Ruohan and Norcia, Anthony and Wu†, Jiajun and Fei-Fei†, Li}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Robot Learning (CoRL)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/bmvc2021-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/bmvc2021-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/bmvc2021-1400.webp"></source> <img src="/assets/img/publication_preview/bmvc2021.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="bmvc2021.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="garg2023visually" class="col-sm-8"> <div class="title">Visually-Guided Audio Spatialization in Video with Geometry-Aware Multi-task Learning</div> <div class="author"> <a href="https://bigharshrag.github.io/" rel="external nofollow noopener" target="_blank">Rishabh Garg</a>, <em>Ruohan Gao</em>, and <a href="https://www.cs.utexas.edu/~grauman/" rel="external nofollow noopener" target="_blank">Kristen Grauman</a> </div> <div class="periodical"> <em>International Journal of Computer Vision (IJCV)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <strong style="border-width:2px;color:red;padding:0.1em">Special Issue for Best Papers of BMVC</strong> <br> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/article/10.1007/s11263-023-01816-8" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://vision.cs.utexas.edu/projects/geometry-aware-binaural/#dataset" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Dataset</a> <a href="https://vision.cs.utexas.edu/projects/geometry-aware-binaural/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">garg2023visually</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Visually-Guided Audio Spatialization in Video with Geometry-Aware Multi-task Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Garg, Rishabh and Gao, Ruohan and Grauman, Kristen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Journal of Computer Vision (IJCV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/of_benchmark_cvpr2023-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/of_benchmark_cvpr2023-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/of_benchmark_cvpr2023-1400.webp"></source> <img src="/assets/img/publication_preview/of_benchmark_cvpr2023.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="of_benchmark_cvpr2023.jpg" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gao2023ObjectFolderBM" class="col-sm-8"> <div class="title">The ObjectFolder Benchmark: Multisensory Object-Centric Learning with Neural and Real Objects</div> <div class="author"> <em>Ruohan Gao*</em>, <a href="https://dou-yiming.github.io/" rel="external nofollow noopener" target="_blank">Yiming Dou*</a>, <a href="https://haolirobo.github.io/" rel="external nofollow noopener" target="_blank">Hao Li*</a>, <a href="https://tanmay-agarwal.com/" rel="external nofollow noopener" target="_blank">Tanmay Agarwal</a>, <a href="http://web.stanford.edu/~bohg/" rel="external nofollow noopener" target="_blank">Jeannette Bohg</a>, <a href="https://yunzhuli.github.io/" rel="external nofollow noopener" target="_blank">Yunzhu Li</a>, <a href="https://profiles.stanford.edu/fei-fei-li" rel="external nofollow noopener" target="_blank">Li Fei-Fei</a>, and <a href="https://jiajunwu.com/" rel="external nofollow noopener" target="_blank">Jiajun Wu</a> </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/papers/realimpact_cvpr2023.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/objectfolder" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://www.youtube.com/watch?v=VhXDempUYgE" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://objectfolder.stanford.edu/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> <a href="https://www.objectfolder.org/swan_vis/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Interactive Demo</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2023ObjectFolderBM</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{The ObjectFolder Benchmark: Multisensory Object-Centric Learning with Neural and Real Objects}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao*, Ruohan and Dou*, Yiming and Li*, Hao and Agarwal, Tanmay and Bohg, Jeannette and Li, Yunzhu and Fei-Fei, Li and Wu, Jiajun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/realimpact_cvpr2023-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/realimpact_cvpr2023-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/realimpact_cvpr2023-1400.webp"></source> <img src="/assets/img/publication_preview/realimpact_cvpr2023.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="realimpact_cvpr2023.jpg" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="clarke2023realimpact" class="col-sm-8"> <div class="title">RealImpact: A Dataset of Impact Sound Fields for Real Objects</div> <div class="author"> <a href="https://samuelpclarke.com/" rel="external nofollow noopener" target="_blank">Samuel Clarke</a>, <em>Ruohan Gao</em>, Mason Wang, <a href="https://ccrma.stanford.edu/~mrau/" rel="external nofollow noopener" target="_blank">Mark Rau</a>, Julia Xu, <a href="https://ccrma.stanford.edu/~mrau/" rel="external nofollow noopener" target="_blank">Mark Rau</a>, <a href="http://juiwang.com/" rel="external nofollow noopener" target="_blank">Jui-Hsien Wang</a>, <a href="http://graphics.stanford.edu/~djames/" rel="external nofollow noopener" target="_blank">Doug James</a>, and <a href="https://jiajunwu.com/" rel="external nofollow noopener" target="_blank">Jiajun Wu</a> </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <strong style="border-width:2px;color:red;padding:0.1em">Highlight</strong> <br> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2306.00956.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/supps/RealImpact_supp" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://github.com/samuel-clarke/RealImpact" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://www.youtube.com/watch?v=q_meSUnzZo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="hhttps://samuelpclarke.com/realimpact/" class="btn btn-sm z-depth-0" role="button">Project Page</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">clarke2023realimpact</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{RealImpact: A Dataset of Impact Sound Fields for Real Objects}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Clarke, Samuel and Gao, Ruohan and Wang, Mason and Rau, Mark and Xu, Julia and Rau, Mark and Wang, Jui-Hsien and James, Doug and Wu, Jiajun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/osf_tmlr2023-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/osf_tmlr2023-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/osf_tmlr2023-1400.webp"></source> <img src="/assets/img/publication_preview/osf_tmlr2023.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="osf_tmlr2023.jpg" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="yu2023osf" class="col-sm-8"> <div class="title">Learning Object-Centric Neural Scattering Functions for Free-Viewpoint Relighting and Scene Composition</div> <div class="author"> <a href="https://kovenyu.com/" rel="external nofollow noopener" target="_blank">Hong-Xing Yu*</a>, <a href="https://shellguo.com/" rel="external nofollow noopener" target="_blank">Michelle Guo*</a>, <a href="https://www.alirezafathi.org/" rel="external nofollow noopener" target="_blank">Alireza Fathi</a>, <a href="https://yuyuchang.github.io/" rel="external nofollow noopener" target="_blank">Yen-Yu Chang</a>, <a href="https://ericryanchan.github.io/index.html" rel="external nofollow noopener" target="_blank">Eric Ryan Chan</a>, <em>Ruohan Gao</em>, <a href="https://www.cs.princeton.edu/~funk/" rel="external nofollow noopener" target="_blank">Thomas Funkhouser</a>, and <a href="https://jiajunwu.com/" rel="external nofollow noopener" target="_blank">Jiajun Wu</a> </div> <div class="periodical"> <em></em> 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2303.06138.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/supps/2023_TMLR_OSF_supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://github.com/michguo/osf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://www.youtube.com/watch?v=BqKiO5GDtH8" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://kovenyu.com/osf/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">yu2023osf</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning Object-Centric Neural Scattering Functions for Free-Viewpoint Relighting and Scene Composition}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yu*, Hong-Xing and Guo*, Michelle and Fathi, Alireza and Chang, Yen-Yu and Chan, Eric Ryan and Gao, Ruohan and Funkhouser, Thomas and Wu, Jiajun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Transactions on Machine Learning Research (TMLR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/dano_ral2023-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/dano_ral2023-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/dano_ral2023-1400.webp"></source> <img src="/assets/img/publication_preview/dano_ral2023.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="dano_ral2023.jpg" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="simon2023ral" class="col-sm-8"> <div class="title">Differentiable Physics Simulation of Dynamics-Augmented Neural Objects</div> <div class="author"> Simon Le Cleac’h, <a href="https://kovenyu.com/" rel="external nofollow noopener" target="_blank">Hong-Xing Yu</a>, <a href="https://shellguo.com/" rel="external nofollow noopener" target="_blank">Michelle Guo</a>, <a href="https://thowell.github.io/" rel="external nofollow noopener" target="_blank">Taylor A. Howell</a>, <em>Ruohan Gao</em>, <a href="https://jiajunwu.com/" rel="external nofollow noopener" target="_blank">Jiajun Wu</a>, <a href="https://www.ri.cmu.edu/ri-faculty/zachary-manchester/" rel="external nofollow noopener" target="_blank">Zachary Manchester</a>, and <a href="http://web.stanford.edu/~schwager/" rel="external nofollow noopener" target="_blank">Mac Schwager</a> </div> <div class="periodical"> <em>Robotics and Automation Letters (RA-L)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2210.09420.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.youtube.com/watch?v=Md0PM-wv_Xg" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">simon2023ral</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Differentiable Physics Simulation of Dynamics-Augmented Neural Objects}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cleac'h, Simon Le and Yu, Hong-Xing and Guo, Michelle and Howell, Taylor A. and Gao, Ruohan and Wu, Jiajun and Manchester, Zachary and Schwager, Mac}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Robotics and Automation Letters (RA-L)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/sonicverse_icra2023-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/sonicverse_icra2023-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/sonicverse_icra2023-1400.webp"></source> <img src="/assets/img/publication_preview/sonicverse_icra2023.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="sonicverse_icra2023.jpg" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gao2023sonicverse" class="col-sm-8"> <div class="title">Sonicverse: A Multisensory Simulation Platform for Training Household Agents that See and Hear</div> <div class="author"> <em>Ruohan Gao*</em>, <a href="https://haolirobo.github.io/" rel="external nofollow noopener" target="_blank">Hao Li*</a>, Gokul Dharan, Zhuzhu Wang, Chengshu Li, Fei Xia, <a href="https://profiles.stanford.edu/silvio-savarese" rel="external nofollow noopener" target="_blank">Silvio Savarese</a>, <a href="https://profiles.stanford.edu/fei-fei-li" rel="external nofollow noopener" target="_blank">Li Fei-Fei</a>, and <a href="https://jiajunwu.com/" rel="external nofollow noopener" target="_blank">Jiajun Wu</a> </div> <div class="periodical"> <em>International Conference on Robotics and Automation (ICRA),</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/papers/gao-icra2023-sonicverse.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/StanfordVL/sonicverse" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://www.youtube.com/watch?v=veqC1K6pxbg" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://ai.stanford.edu/\~rhgao/sonicverse/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2023sonicverse</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Sonicverse: A Multisensory Simulation Platform for Training Household Agents that See and Hear}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao*, Ruohan and Li*, Hao and Dharan, Gokul and Wang, Zhuzhu and Li, Chengshu and Xia, Fei and Savarese, Silvio and Fei-Fei, Li and Wu, Jiajun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Robotics and Automation (ICRA),}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/emma_dataset_iclr2023-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/emma_dataset_iclr2023-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/emma_dataset_iclr2023-1400.webp"></source> <img src="/assets/img/publication_preview/emma_dataset_iclr2023.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="emma_dataset_iclr2023.jpg" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="standley2023emma" class="col-sm-8"> <div class="title">An Extensible Multi-modal Multi-task Object Dataset with Materials</div> <div class="author"> Trevor Scott Standley, <em>Ruohan Gao</em>, Dawn Chen, <a href="https://jiajunwu.com/" rel="external nofollow noopener" target="_blank">Jiajun Wu</a>, and <a href="https://profiles.stanford.edu/silvio-savarese" rel="external nofollow noopener" target="_blank">Silvio Savarese</a> </div> <div class="periodical"> <em>International Conference on Learning Representations (ICLR)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/papers/emma_iclr2023.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://docs.google.com/forms/d/e/1FAIpQLScOO2YEeuOPyCusWx6KKS1kt8hGe6CCn2mtxr17yimTLe96qw/viewform" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Dataset</a> <a href="https://emma.stanford.edu/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> <a href="https://emma-app.stanford.edu/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Interactive Demo</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">standley2023emma</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{An Extensible Multi-modal Multi-task Object Dataset with Materials}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Standley, Trevor Scott and Gao, Ruohan and Chen, Dawn and Wu, Jiajun and Savarese, Silvio}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Learning Representations (ICLR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/see_hear_feel_corl2022-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/see_hear_feel_corl2022-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/see_hear_feel_corl2022-1400.webp"></source> <img src="/assets/img/publication_preview/see_hear_feel_corl2022.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="see_hear_feel_corl2022.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="li2022seehearfeel" class="col-sm-8"> <div class="title">See, Hear, and Feel: Smart Sensory Fusion for Robotic Manipulation</div> <div class="author"> <a href="https://haolirobo.github.io/" rel="external nofollow noopener" target="_blank">Hao Li*</a>, Yizhi Zhang*, Junzhe Zhu, <a href="http://shaoxiongwang.com/" rel="external nofollow noopener" target="_blank">Shaoxiong Wang</a>, <a href="https://scholar.google.com/citations?hl=en&amp;user=2Dmb3XYAAAAJ" rel="external nofollow noopener" target="_blank">Michelle A. Lee</a>, <a href="http://hxu.rocks/" rel="external nofollow noopener" target="_blank">Huazhe Xu</a>, <a href="http://persci.mit.edu/people/adelson" rel="external nofollow noopener" target="_blank">Edward Adelson</a>, <a href="https://profiles.stanford.edu/fei-fei-li" rel="external nofollow noopener" target="_blank">Li Fei-Fei</a>, <em>Ruohan Gao†</em>, and <a href="https://jiajunwu.com/" rel="external nofollow noopener" target="_blank">Jiajun Wu†</a> </div> <div class="periodical"> <em>Conference on Robot Learning (CoRL)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2212.03858.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/supps/seel_hear_feel_supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://www.youtube.com/watch?v=sRdx3sa6ryk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://ai.stanford.edu/\~rhgao/see_hear_feel/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">li2022seehearfeel</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{See, Hear, and Feel: Smart Sensory Fusion for Robotic Manipulation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li*, Hao and Zhang*, Yizhi and Zhu, Junzhe and Wang, Shaoxiong and Lee, Michelle A. and Xu, Huazhe and Adelson, Edward and Fei-Fei, Li and Gao†, Ruohan and Wu†, Jiajun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Robot Learning (CoRL)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/objectfolderV2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/objectfolderV2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/objectfolderV2-1400.webp"></source> <img src="/assets/img/publication_preview/objectfolderV2.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="objectfolderV2.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gao2022ObjectFolderV2" class="col-sm-8"> <div class="title">ObjectFolder 2.0: A Multisensory Object Dataset for Sim2Real Transfer</div> <div class="author"> <em>Ruohan Gao*</em>, <a href="https://si-lynnn.github.io/" rel="external nofollow noopener" target="_blank">Zilin Si*</a>, <a href="https://yuyuchang.github.io/" rel="external nofollow noopener" target="_blank">Yen-Yu Chang*</a>, <a href="https://samuelpclarke.com/" rel="external nofollow noopener" target="_blank">Samuel Clarke</a>, <a href="http://web.stanford.edu/~bohg/" rel="external nofollow noopener" target="_blank">Jeannette Bohg</a>, <a href="https://profiles.stanford.edu/fei-fei-li" rel="external nofollow noopener" target="_blank">Li Fei-Fei</a>, <a href="http://robotouch.ri.cmu.edu/yuanwz/" rel="external nofollow noopener" target="_blank">Wenzhen Yuan</a>, and <a href="https://jiajunwu.com/" rel="external nofollow noopener" target="_blank">Jiajun Wu</a> </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2204.02389.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/supps/ObjectFolderV2_Supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://github.com/rhgao/ObjectFolder" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Dataset</a> <a href="https://ai.stanford.edu/\~rhgao/objectfolder2.0/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2022ObjectFolderV2</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ObjectFolder 2.0: A Multisensory Object Dataset for Sim2Real Transfer}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao*, Ruohan and Si*, Zilin and Chang*, Yen-Yu and Clarke, Samuel and Bohg, Jeannette and Fei-Fei, Li and Yuan, Wenzhen and Wu, Jiajun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/visual_acoustic_matching_cvpr2022-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/visual_acoustic_matching_cvpr2022-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/visual_acoustic_matching_cvpr2022-1400.webp"></source> <img src="/assets/img/publication_preview/visual_acoustic_matching_cvpr2022.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="visual_acoustic_matching_cvpr2022.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="chen2022visual" class="col-sm-8"> <div class="title">Visual Acoustic Matching</div> <div class="author"> <a href="https://changan.io/" rel="external nofollow noopener" target="_blank">Changan Chen</a>, <em>Ruohan Gao</em>, <a href="https://scholar.google.com/citations?hl=en&amp;user=S_S6SZAAAAAJ" rel="external nofollow noopener" target="_blank">Paul Calamia</a>, and <a href="https://www.cs.utexas.edu/~grauman/" rel="external nofollow noopener" target="_blank">Kristen Grauman</a> </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <strong style="border-width:2px;color:red;padding:0.1em">Oral Presentation</strong> <br> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2202.06875.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/facebookresearch/visual-acoustic-matching" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://vision.cs.utexas.edu/projects/visual-acoustic-matching/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> <a href="https://www.engadget.com/metas-latest-auditory-ai-promise-a-more-immersive-ar-vr-experience-130029625.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Media Coverage</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">chen2022visual</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Visual Acoustic Matching}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chen, Changan and Gao, Ruohan and Calamia, Paul and Grauman, Kristen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/objectfolder_corl2021-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/objectfolder_corl2021-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/objectfolder_corl2021-1400.webp"></source> <img src="/assets/img/publication_preview/objectfolder_corl2021.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="objectfolder_corl2021.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gao2021ObjectFolder" class="col-sm-8"> <div class="title">ObjectFolder: A Dataset of Objects with Implicit Visual, Auditory, and Tactile Representations</div> <div class="author"> <em>Ruohan Gao</em>, <a href="https://yuyuchang.github.io/" rel="external nofollow noopener" target="_blank">Yen-Yu Chang</a>, Shivani Mall, <a href="https://profiles.stanford.edu/fei-fei-li" rel="external nofollow noopener" target="_blank">Li Fei-Fei</a>, and <a href="https://jiajunwu.com/" rel="external nofollow noopener" target="_blank">Jiajun Wu</a> </div> <div class="periodical"> <em>Conference on Robot Learning (CoRL)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2109.07991.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/supps/ObjectFolder_Supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://github.com/rhgao/ObjectFolder" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Dataset</a> <a href="https://ai.stanford.edu/\~rhgao/objectfolder/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2021ObjectFolder</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ObjectFolder: A Dataset of Objects with Implicit Visual, Auditory, and Tactile Representations}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao, Ruohan and Chang, Yen-Yu and Mall, Shivani and Fei-Fei, Li and Wu, Jiajun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Robot Learning (CoRL)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/diffImpact_corl2021-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/diffImpact_corl2021-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/diffImpact_corl2021-1400.webp"></source> <img src="/assets/img/publication_preview/diffImpact_corl2021.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="diffImpact_corl2021.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="clarke2021diffimpact" class="col-sm-8"> <div class="title">DiffImpact: Differentiable Rendering and Identification of Impact Sounds</div> <div class="author"> <a href="https://samuelpclarke.com/" rel="external nofollow noopener" target="_blank">Samuel Clarke</a>, <a href="https://scholar.google.com/citations?user=dFdEHskAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Negin Heravi</a>, <a href="https://ccrma.stanford.edu/~mrau/" rel="external nofollow noopener" target="_blank">Mark Rau</a>, <em>Ruohan Gao</em>, <a href="https://jiajunwu.com/" rel="external nofollow noopener" target="_blank">Jiajun Wu</a>, <a href="http://graphics.stanford.edu/~djames/" rel="external nofollow noopener" target="_blank">Doug James</a>, and <a href="http://web.stanford.edu/~bohg/" rel="external nofollow noopener" target="_blank">Jeannette Bohg</a> </div> <div class="periodical"> <em>Conference on Robot Learning (CoRL)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <strong style="border-width:2px;color:red;padding:0.1em">Oral Presentation</strong> <br> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/papers/clarke-corl2021.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/supps/diffImpact_supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://github.com/samuel-clarke/diffimpact" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://www.youtube.com/watch?v=mnfzQeBUw6A" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://sites.google.com/view/diffimpact" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">clarke2021diffimpact</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DiffImpact: Differentiable Rendering and Identification of Impact Sounds}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Clarke, Samuel and Heravi, Negin and Rau, Mark and Gao, Ruohan and Wu, Jiajun and James, Doug and Bohg, Jeannette}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Robot Learning (CoRL)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/bmvc2021-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/bmvc2021-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/bmvc2021-1400.webp"></source> <img src="/assets/img/publication_preview/bmvc2021.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="bmvc2021.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="garg2021geometry" class="col-sm-8"> <div class="title">Geometry-Aware Multi-Task Learning for Binaural Audio Generation from Video</div> <div class="author"> <a href="https://bigharshrag.github.io/" rel="external nofollow noopener" target="_blank">Rishabh Garg</a>, <em>Ruohan Gao</em>, and <a href="https://www.cs.utexas.edu/~grauman/" rel="external nofollow noopener" target="_blank">Kristen Grauman</a> </div> <div class="periodical"> <em>British Machine Vision Conference (BMVC)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <strong style="border-width:2px;color:red;padding:0.1em">Best Paper Award Runner-Up</strong> <br> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2111.10882.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/supps/bmvc2021_supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://vision.cs.utexas.edu/projects/geometry-aware-binaural/#dataset" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Dataset</a> <a href="https://vision.cs.utexas.edu/projects/geometry-aware-binaural/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">garg2021geometry</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Geometry-Aware Multi-Task Learning for Binaural Audio Generation from Video}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Garg, Rishabh and Gao, Ruohan and Grauman, Kristen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{British Machine Vision Conference (BMVC)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/thesis_teaser-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/thesis_teaser-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/thesis_teaser-1400.webp"></source> <img src="/assets/img/publication_preview/thesis_teaser.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="thesis_teaser.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gao2021dissertation" class="col-sm-8"> <div class="title">Look and Listen: From Semantic to Spatial Audio-Visual Perception</div> <div class="author"> <em>Ruohan Gao</em> </div> <div class="periodical"> <em>Ph.D. Dissertation</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <strong style="border-width:2px;color:red;padding:0.1em">Michael H. Granof Award, UT Austin’s Top 1 Doctoral Dissertation</strong> <br> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/papers/Ruohan_Gao_dissertation.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://gradschool.utexas.edu/news/graduate-school-announces-2021-award-winners" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Media Coverage</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2021dissertation</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Look and Listen: From Semantic to Spatial Audio-Visual Perception}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao, Ruohan}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Ph.D. Dissertation}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">media</span> <span class="p">=</span> <span class="s">{https://gradschool.utexas.edu/news/graduate-school-announces-2021-award-winners}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/VisualVoice_cvpr2021-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/VisualVoice_cvpr2021-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/VisualVoice_cvpr2021-1400.webp"></source> <img src="/assets/img/publication_preview/VisualVoice_cvpr2021.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="VisualVoice_cvpr2021.jpg" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gao2021visualvoice" class="col-sm-8"> <div class="title">Visualvoice: Audio-visual speech separation with cross-modal consistency</div> <div class="author"> <em>Ruohan Gao</em>, and <a href="https://www.cs.utexas.edu/~grauman/" rel="external nofollow noopener" target="_blank">Kristen Grauman</a> </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/papers/gao2021VisualVoice.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/supps/VisualVoice_Supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://github.com/facebookresearch/VisualVoice" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://vision.cs.utexas.edu/projects/VisualVoice/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> <a href="https://ai.meta.com/blog/ai-driven-acoustic-synthesis-for-augmented-and-virtual-reality-experiences/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Media Coverage</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2021visualvoice</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Visualvoice: Audio-visual speech separation with cross-modal consistency}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao, Ruohan and Grauman, Kristen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/av_wan_iclr2021-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/av_wan_iclr2021-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/av_wan_iclr2021-1400.webp"></source> <img src="/assets/img/publication_preview/av_wan_iclr2021.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="av_wan_iclr2021.jpg" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="chen2021waypoints" class="col-sm-8"> <div class="title">Learning to Set Waypoints for Audio-Visual Navigation</div> <div class="author"> <a href="https://changan.io/" rel="external nofollow noopener" target="_blank">Changan Chen</a>, <a href="https://sagnikmjr.github.io/" rel="external nofollow noopener" target="_blank">Sagnik Majumder</a>, <a href="https://users.cs.utah.edu/~ziad/" rel="external nofollow noopener" target="_blank">Ziad Al-Halah</a>, <em>Ruohan Gao</em>, <a href="https://srama2512.github.io/" rel="external nofollow noopener" target="_blank">Santhosh Kumar Ramakrishnan</a>, and <a href="https://www.cs.utexas.edu/~grauman/" rel="external nofollow noopener" target="_blank">Kristen Grauman</a> </div> <div class="periodical"> <em>International Conference on Learning Representations (ICLR)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2008.09622.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/facebookresearch/sound-spaces/tree/main/ss_baselines/av_wan" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://vision.cs.utexas.edu/projects/audio_visual_waypoints/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">chen2021waypoints</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning to Set Waypoints for Audio-Visual Navigation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chen, Changan and Majumder, Sagnik and Al-Halah, Ziad and Gao, Ruohan and Ramakrishnan, Santhosh Kumar and Grauman, Kristen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Learning Representations (ICLR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/visualEchoes_eccv2020-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/visualEchoes_eccv2020-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/visualEchoes_eccv2020-1400.webp"></source> <img src="/assets/img/publication_preview/visualEchoes_eccv2020.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="visualEchoes_eccv2020.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gao2020visualechoes" class="col-sm-8"> <div class="title">VisualEchoes: Spatial Visual Representation Learning through Echolocation</div> <div class="author"> <em>Ruohan Gao</em>, <a href="https://changan.io/" rel="external nofollow noopener" target="_blank">Changan Chen</a>, <a href="https://users.cs.utah.edu/~ziad/" rel="external nofollow noopener" target="_blank">Ziad Al-Halah</a>, <a href="https://scholar.google.com/citations?user=YcL_y8AAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Carl Schissler</a>, and <a href="https://www.cs.utexas.edu/~grauman/" rel="external nofollow noopener" target="_blank">Kristen Grauman</a> </div> <div class="periodical"> <em>European Conference on Computer Vision (ECCV)</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/papers/gao-eccv2020-visualechoes.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/supps/visualechoes_supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://github.com/facebookresearch/VisualEchoes" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Dataset</a> <a href="https://vision.cs.utexas.edu/projects/visualEchoes/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2020visualechoes</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{VisualEchoes: Spatial Visual Representation Learning through Echolocation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao, Ruohan and Chen, Changan and Al-Halah, Ziad and Schissler, Carl and Grauman, Kristen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{European Conference on Computer Vision (ECCV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/listen_to_look_cvpr2020-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/listen_to_look_cvpr2020-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/listen_to_look_cvpr2020-1400.webp"></source> <img src="/assets/img/publication_preview/listen_to_look_cvpr2020.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="listen_to_look_cvpr2020.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gao2020listentolook" class="col-sm-8"> <div class="title">Listen to Look: Action Recognition by Previewing Audio</div> <div class="author"> <em>Ruohan Gao</em>, <a href="https://ami.postech.ac.kr/members/tae-hyun-oh" rel="external nofollow noopener" target="_blank">Tae-Hyun Oh</a>, <a href="https://www.cs.utexas.edu/~grauman/" rel="external nofollow noopener" target="_blank">Kristen Grauman</a>, and <a href="https://ltorresa.github.io/home.html" rel="external nofollow noopener" target="_blank">Lorenzo Torresani</a> </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/papers/gao-cvpr2020-listen-to-look.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/supps/listen-to-look-cvpr2020-supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://github.com/facebookresearch/Listen-to-Look" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/posters/listen-to-look-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://vision.cs.utexas.edu/projects/listen_to_look/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2020listentolook</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Listen to Look: Action Recognition by Previewing Audio}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao, Ruohan and Oh, Tae-Hyun and Grauman, Kristen and Torresani, Lorenzo}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/co-separation-iccv2019-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/co-separation-iccv2019-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/co-separation-iccv2019-1400.webp"></source> <img src="/assets/img/publication_preview/co-separation-iccv2019.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="co-separation-iccv2019.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gao2019coseparation" class="col-sm-8"> <div class="title">Co-Separating Sounds of Visual Objects</div> <div class="author"> <em>Ruohan Gao</em>, and <a href="https://www.cs.utexas.edu/~grauman/" rel="external nofollow noopener" target="_blank">Kristen Grauman</a> </div> <div class="periodical"> <em>International Conference on Computer Vision (ICCV)</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/papers/coseparation-iccv2019.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/supps/coseparation-iccv2019-supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://github.com/rhgao/co-separation" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/posters/coseparation-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://vision.cs.utexas.edu/projects/coseparation/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2019coseparation</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Co-Separating Sounds of Visual Objects}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao, Ruohan and Grauman, Kristen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Computer Vision (ICCV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/2.5D_visual_sound_cvpr2019-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/2.5D_visual_sound_cvpr2019-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/2.5D_visual_sound_cvpr2019-1400.webp"></source> <img src="/assets/img/publication_preview/2.5D_visual_sound_cvpr2019.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="2.5D_visual_sound_cvpr2019.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gao2019visual-sound" class="col-sm-8"> <div class="title">2.5D Visual Sound</div> <div class="author"> <em>Ruohan Gao</em>, and <a href="https://www.cs.utexas.edu/~grauman/" rel="external nofollow noopener" target="_blank">Kristen Grauman</a> </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> <strong style="border-width:2px;color:red;padding:0.1em">Best Paper Award Finalist</strong> <br> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/papers/gao-cvpr2019.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/supps/gao-cvpr2019-supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://github.com/facebookresearch/2.5D-Visual-Sound" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://github.com/facebookresearch/FAIR-Play" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Dataset</a> <a href="https://www.youtube.com/watch?v=JwaBi_2JFeU&amp;t=2805s" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://vision.cs.utexas.edu/projects/2.5D_visual_sound/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> <a href="https://www.technologyreview.com/2018/12/26/138088/deep-learning-turns-mono-recordings-into-immersive-sound/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Media Coverage</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2019visual-sound</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{2.5D Visual Sound}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao, Ruohan and Grauman, Kristen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">media</span> <span class="p">=</span> <span class="s">{https://www.technologyreview.com/2018/12/26/138088/deep-learning-turns-mono-recordings-into-immersive-sound/}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/audioobjects_eccv2018-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/audioobjects_eccv2018-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/audioobjects_eccv2018-1400.webp"></source> <img src="/assets/img/publication_preview/audioobjects_eccv2018.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="audioobjects_eccv2018.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gao2018object-sounds" class="col-sm-8"> <div class="title">Learning to Separate Object Sounds by Watching Unlabeled Video</div> <div class="author"> <em>Ruohan Gao</em>, <a href="https://www.rogerioferis.org/" rel="external nofollow noopener" target="_blank">Rogerio Feris</a>, and <a href="https://www.cs.utexas.edu/~grauman/" rel="external nofollow noopener" target="_blank">Kristen Grauman</a> </div> <div class="periodical"> <em>European Conference on Computer Vision (ECCV)</em>, 2018 </div> <div class="periodical"> </div> <div class="links"> <strong style="border-width:2px;color:red;padding:0.1em">Oral Presentation</strong> <br> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/papers/sound-sep-eccv2018.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/supps/sound-sep-eccv2018_supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://github.com/rhgao/Deep-MIML-Network" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://www.youtube.com/watch?v=SDKEUdp6edc" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="/assets/posters/gao-eccv2018-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://vision.cs.utexas.edu/projects/separating_object_sounds/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2018object-sounds</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning to Separate Object Sounds by Watching Unlabeled Video}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao, Ruohan and Feris, Rogerio and Grauman, Kristen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{European Conference on Computer Vision (ECCV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/shapecodes_eccv2018-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/shapecodes_eccv2018-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/shapecodes_eccv2018-1400.webp"></source> <img src="/assets/img/publication_preview/shapecodes_eccv2018.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="shapecodes_eccv2018.jpg" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="jayaraman2018shape" class="col-sm-8"> <div class="title">ShapeCodes: Self-Supervised Feature Learning by Lifting Views to Viewgrids</div> <div class="author"> <a href="https://www.seas.upenn.edu/~dineshj/" rel="external nofollow noopener" target="_blank">Dinesh Jayaraman</a>, <em>Ruohan Gao</em>, and <a href="https://www.cs.utexas.edu/~grauman/" rel="external nofollow noopener" target="_blank">Kristen Grauman</a> </div> <div class="periodical"> <em>European Conference on Computer Vision (ECCV)</em>, 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/papers/shape-codes-eccv2018.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/supps/shape-codes-eccv2018-supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">jayaraman2018shape</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ShapeCodes: Self-Supervised Feature Learning by Lifting Views to Viewgrids}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jayaraman, Dinesh and Gao, Ruohan and Grauman, Kristen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{European Conference on Computer Vision (ECCV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/im2flow_cvpr2018-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/im2flow_cvpr2018-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/im2flow_cvpr2018-1400.webp"></source> <img src="/assets/img/publication_preview/im2flow_cvpr2018.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="im2flow_cvpr2018.jpg" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gao2018im2flow" class="col-sm-8"> <div class="title">Im2Flow: Motion Hallucination from Static Images for Action Recognition</div> <div class="author"> <em>Ruohan Gao</em>, <a href="https://scholar.google.com/citations?user=ZFW98t4AAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Bo Xiong</a>, and <a href="https://www.cs.utexas.edu/~grauman/" rel="external nofollow noopener" target="_blank">Kristen Grauman</a> </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2018 </div> <div class="periodical"> </div> <div class="links"> <strong style="border-width:2px;color:red;padding:0.1em">Oral Presentation</strong> <br> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/papers/gao-cvpr2018.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/supps/gao-cvpr2018-supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://github.com/rhgao/Im2Flow" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://www.youtube.com/watch?v=GBo4sFNzhtU&amp;t=581s" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="/assets/posters/gao-cvpr2018-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://vision.cs.utexas.edu/projects/im2flow/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2018im2flow</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Im2Flow: Motion Hallucination from Static Images for Action Recognition}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao, Ruohan and Xiong, Bo and Grauman, Kristen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/ondemand_iccv2017-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/ondemand_iccv2017-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/ondemand_iccv2017-1400.webp"></source> <img src="/assets/img/publication_preview/ondemand_iccv2017.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="ondemand_iccv2017.jpg" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gao2017on-demand" class="col-sm-8"> <div class="title">On-Demand Learning for Deep Image Restoration</div> <div class="author"> <em>Ruohan Gao</em>, and <a href="https://www.cs.utexas.edu/~grauman/" rel="external nofollow noopener" target="_blank">Kristen Grauman</a> </div> <div class="periodical"> <em>International Conference on Computer Vision (ICCV)</em>, 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/papers/gao-iccv2017.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/supps/gao-iccv2017-supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://github.com/rhgao/on-demand-learning/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/posters/gao-iccv2017-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://vision.cs.utexas.edu/projects/on_demand_learning/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2017on-demand</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{On-Demand Learning for Deep Image Restoration}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao, Ruohan and Grauman, Kristen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Computer Vision (ICCV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2016</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/objectcentric_accv2016-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/objectcentric_accv2016-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/objectcentric_accv2016-1400.webp"></source> <img src="/assets/img/publication_preview/objectcentric_accv2016.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="objectcentric_accv2016.jpg" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gao2016object-centric" class="col-sm-8"> <div class="title">Object-Centric Representation Learning from Unlabeled Videos</div> <div class="author"> <em>Ruohan Gao</em>, <a href="https://www.seas.upenn.edu/~dineshj/" rel="external nofollow noopener" target="_blank">Dinesh Jayaraman</a>, and <a href="https://www.cs.utexas.edu/~grauman/" rel="external nofollow noopener" target="_blank">Kristen Grauman</a> </div> <div class="periodical"> <em>Asian Conference on Computer Vision (ACCV)</em>, 2016 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/papers/gao-accv2016.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/posters/gao-accv2016-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://vision.cs.utexas.edu/projects/object_centric_unsup/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2016object-centric</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Object-Centric Representation Learning from Unlabeled Videos}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao, Ruohan and Jayaraman, Dinesh and Grauman, Kristen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Asian Conference on Computer Vision (ACCV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">IEEE ICC</abbr></div> <div id="gao2016accelerating" class="col-sm-8"> <div class="title">Accelerating Graph Mining Algorithms via Uniform Random Edge Sampling</div> <div class="author"> <em>Ruohan Gao</em>, <a href="https://www.fst.um.edu.mo/personal/huanlexu/" rel="external nofollow noopener" target="_blank">Huanle Xu</a>, <a href="https://hupili.net/" rel="external nofollow noopener" target="_blank">Pili Hu</a>, and <a href="https://staff.ie.cuhk.edu.hk/~wclau/" rel="external nofollow noopener" target="_blank">Wing Cheong Lau</a> </div> <div class="periodical"> <em>IEEE International Conference on Communications (ICC)</em>, 2016 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/papers/graphsampling_icc2016.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2016accelerating</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Accelerating Graph Mining Algorithms via Uniform Random Edge Sampling}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao, Ruohan and Xu, Huanle and Hu, Pili and and Lau, Wing Cheong}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE International Conference on Communications (ICC)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2015</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">IEEE GLOBECOM</abbr></div> <div id="gao2015graph" class="col-sm-8"> <div class="title">Graph Property Preservation under Community-Based Sampling</div> <div class="author"> <em>Ruohan Gao</em>, <a href="https://hupili.net/" rel="external nofollow noopener" target="_blank">Pili Hu</a>, and <a href="https://staff.ie.cuhk.edu.hk/~wclau/" rel="external nofollow noopener" target="_blank">Wing Cheong Lau</a> </div> <div class="periodical"> <em>IEEE Global Communications Conference (GLOBECOM)</em>, 2015 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/papers/CBS_globecom2015.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2015graph</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Graph Property Preservation under Community-Based Sampling}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao, Ruohan and Hu, Pili and and Lau, Wing Cheong}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE Global Communications Conference (GLOBECOM)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2015}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Ruohan Gao. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>
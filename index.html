<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Ruohan Gao</title> <meta name="author" content="Ruohan Gao"> <meta name="description" content="Homepage for Ruohan Gao "> <meta name="keywords" content="Ruohan Gao, Homepage, Computer Vision, Machine Learning, Artificial Intelligence, AI"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/umd.png"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ruohangao.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%72%68%67%61%6F@%75%6D%64.%65%64%75" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=i02oEgMAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/rhgao" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://twitter.com/RuohanGao1" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">Home<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching</a> </li> <li class="nav-item"> <a class="nav-link" href="/assets/Ruohan_Gao_CV.pdf">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <article> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Ruohan-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Ruohan-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Ruohan-1400.webp"></source> <img src="/assets/img/Ruohan.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="Ruohan.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-9"> <h1 class="post-title"> Ruohan Gao </h1> <p class="desc"><b>Research Scientist</b> <br> <a href="https://about.meta.com/realitylabs/" rel="external nofollow noopener" target="_blank">Meta Reality Labs</a> <br><br> Incoming <b>Assistant Professor</b> <br> <a href="https://www.cs.umd.edu/" rel="external nofollow noopener" target="_blank">Department of Computer Science</a>, <a href="https://umd.edu/" rel="external nofollow noopener" target="_blank">University of Maryland, College Park</a> <br><br> Email: rhgao[AT]umd.edu </p> </div> </div> <div class="clearfix"> <p>I am currently a Research Scientist at Meta Reality Labs. Previously, I received my Ph.D. in Computer Science from <a href="http://www.utexas.edu/" rel="external nofollow noopener" target="_blank">The University of Texas at Austin</a> advised by <a href="http://www.cs.utexas.edu/~grauman" rel="external nofollow noopener" target="_blank">Kristen Grauman</a>, and then spent two years as a PostDoc at <a href="https://svl.stanford.edu/" rel="external nofollow noopener" target="_blank">Stanford Vision and Learning Lab</a> working with <a href="https://profiles.stanford.edu/fei-fei-li" rel="external nofollow noopener" target="_blank">Fei-Fei Li</a>, <a href="https://jiajunwu.com/" rel="external nofollow noopener" target="_blank">Jiajun Wu</a>, and <a href="https://profiles.stanford.edu/silvio-savarese" rel="external nofollow noopener" target="_blank">Silvio Savarese</a>.</p> <p>My research primarily focuses on computer vision and machine learning, with a particular emphasis on multisensory learning involving sight, sound, and touch. The overarching goal of my research is to enpower machines to emulate and enhance human capabilities in seeing, hearing, and feeling, ultimately enabling them to comprehensively perceive, understand, and engage with the intricacies of the multisensory world.</p> <p><strong><em><span style="color:red">Prospective Students:</span></em></strong> I am actively seeking self-motivated Ph.D. students to join my group starting Fall 2024. If you are interested in working with me, please apply <a href="https://www.cs.umd.edu/grad/catalog" rel="external nofollow noopener" target="_blank">here</a> and mention my name in your application.</p> </div> <h2><a href="/news/" style="color: inherit;">News</a></h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row"><li></th> <td> <strong><em>I will be joining the <a href="https://www.cs.umd.edu/" rel="external nofollow noopener" target="_blank">Department of Computer Science</a> at <a href="https://umd.edu/" rel="external nofollow noopener" target="_blank">University of Maryland, College Park (UMD)</a> as an Assistant Professor late 2024.</em></strong> </td> </tr> <tr> <th scope="row"><li></th> <td> I serve as an Area Chair for ICCV 2023, and a SPC for AAAI 2023, 2024. </td> </tr> <tr> <th scope="row"><li></th> <td> We are organizing the <a href="https://av4d.org/" rel="external nofollow noopener" target="_blank">AV4D Workshop</a> at ICCV 2023. </td> </tr> <tr> <th scope="row"><li></th> <td> We are organizing the <a href="https://sightsound.org/" rel="external nofollow noopener" target="_blank">Sight and Sound Workshop</a> at CVPR 2023. </td> </tr> <tr> <th scope="row"><li></th> <td> We are organizing the <a href="https://creativeai-ws.github.io/" rel="external nofollow noopener" target="_blank">Creative AI Across Modalities Workshop</a> at AAAI 2023. </td> </tr> <tr> <th scope="row"><li></th> <td> We are organizing the <a href="https://eml-workshop.github.io/" rel="external nofollow noopener" target="_blank">Embodied Multimodal Learning Workshop</a> at ICLR 2021. </td> </tr> <tr> <th scope="row"><li></th> <td> I am very honored to have received the <a href="https://gradschool.utexas.edu/news/graduate-school-announces-2021-award-winners" rel="external nofollow noopener" target="_blank">Michael H. Granof Award</a> that recognizes UT Austin’s Top 1 Doctoral Dissertation of 2021. </td> </tr> </table> </div> </div> <h2>Selected Publications <font size="5"><a href="/publications/">[full list]</a></font> </h2> <div class="publications"> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/of_benchmark_cvpr2023-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/of_benchmark_cvpr2023-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/of_benchmark_cvpr2023-1400.webp"></source> <img src="/assets/img/publication_preview/of_benchmark_cvpr2023.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="of_benchmark_cvpr2023.jpg" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gao2023ObjectFolderBM" class="col-sm-8"> <div class="title">The ObjectFolder Benchmark: Multisensory Object-Centric Learning with Neural and Real Objects</div> <div class="author"> <em>Ruohan Gao*</em>, <a href="https://dou-yiming.github.io/" rel="external nofollow noopener" target="_blank">Yiming Dou*</a>, <a href="https://haolirobo.github.io/" rel="external nofollow noopener" target="_blank">Hao Li*</a>, <a href="https://tanmay-agarwal.com/" rel="external nofollow noopener" target="_blank">Tanmay Agarwal</a>, <a href="http://web.stanford.edu/~bohg/" rel="external nofollow noopener" target="_blank">Jeannette Bohg</a>, <a href="https://yunzhuli.github.io/" rel="external nofollow noopener" target="_blank">Yunzhu Li</a>, <a href="https://profiles.stanford.edu/fei-fei-li" rel="external nofollow noopener" target="_blank">Li Fei-Fei</a>, and <a href="https://jiajunwu.com/" rel="external nofollow noopener" target="_blank">Jiajun Wu</a> </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2306.00956.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/objectfolder" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://www.youtube.com/watch?v=VhXDempUYgE" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://objectfolder.stanford.edu/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> <a href="https://www.objectfolder.org/swan_vis/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Interactive Demo</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2023ObjectFolderBM</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{The ObjectFolder Benchmark: Multisensory Object-Centric Learning with Neural and Real Objects}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao*, Ruohan and Dou*, Yiming and Li*, Hao and Agarwal, Tanmay and Bohg, Jeannette and Li, Yunzhu and Fei-Fei, Li and Wu, Jiajun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/realimpact_cvpr2023-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/realimpact_cvpr2023-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/realimpact_cvpr2023-1400.webp"></source> <img src="/assets/img/publication_preview/realimpact_cvpr2023.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="realimpact_cvpr2023.jpg" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="clarke2023realimpact" class="col-sm-8"> <div class="title">RealImpact: A Dataset of Impact Sound Fields for Real Objects</div> <div class="author"> <a href="https://samuelpclarke.com/" rel="external nofollow noopener" target="_blank">Samuel Clarke</a>, <em>Ruohan Gao</em>, <a href="https://masonlwang.com/" rel="external nofollow noopener" target="_blank">Mason Wang</a>, <a href="https://ccrma.stanford.edu/~mrau/" rel="external nofollow noopener" target="_blank">Mark Rau</a>, Julia Xu, <a href="https://ccrma.stanford.edu/~mrau/" rel="external nofollow noopener" target="_blank">Mark Rau</a>, <a href="http://juiwang.com/" rel="external nofollow noopener" target="_blank">Jui-Hsien Wang</a>, <a href="http://graphics.stanford.edu/~djames/" rel="external nofollow noopener" target="_blank">Doug James</a>, and <a href="https://jiajunwu.com/" rel="external nofollow noopener" target="_blank">Jiajun Wu</a> </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <strong style="border-width:2px;color:red;padding:0.1em">Highlight Paper</strong> <br> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2306.09944.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/supps/RealImpact_supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://github.com/samuel-clarke/RealImpact" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://www.youtube.com/watch?v=q_meSUnzZo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="hhttps://samuelpclarke.com/realimpact/" class="btn btn-sm z-depth-0" role="button">Project Page</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">clarke2023realimpact</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{RealImpact: A Dataset of Impact Sound Fields for Real Objects}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Clarke, Samuel and Gao, Ruohan and Wang, Mason and Rau, Mark and Xu, Julia and Rau, Mark and Wang, Jui-Hsien and James, Doug and Wu, Jiajun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/see_hear_feel_corl2022-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/see_hear_feel_corl2022-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/see_hear_feel_corl2022-1400.webp"></source> <img src="/assets/img/publication_preview/see_hear_feel_corl2022.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="see_hear_feel_corl2022.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="li2022seehearfeel" class="col-sm-8"> <div class="title">See, Hear, and Feel: Smart Sensory Fusion for Robotic Manipulation</div> <div class="author"> <a href="https://haolirobo.github.io/" rel="external nofollow noopener" target="_blank">Hao Li*</a>, Yizhi Zhang*, Junzhe Zhu, <a href="http://shaoxiongwang.com/" rel="external nofollow noopener" target="_blank">Shaoxiong Wang</a>, <a href="https://scholar.google.com/citations?hl=en&amp;user=2Dmb3XYAAAAJ" rel="external nofollow noopener" target="_blank">Michelle A. Lee</a>, <a href="http://hxu.rocks/" rel="external nofollow noopener" target="_blank">Huazhe Xu</a>, <a href="http://persci.mit.edu/people/adelson" rel="external nofollow noopener" target="_blank">Edward Adelson</a>, <a href="https://profiles.stanford.edu/fei-fei-li" rel="external nofollow noopener" target="_blank">Li Fei-Fei</a>, <em>Ruohan Gao†</em>, and <a href="https://jiajunwu.com/" rel="external nofollow noopener" target="_blank">Jiajun Wu†</a> </div> <div class="periodical"> <em>Conference on Robot Learning (CoRL)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2212.03858.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/supps/seel_hear_feel_supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://www.youtube.com/watch?v=sRdx3sa6ryk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://ai.stanford.edu/\~rhgao/see_hear_feel/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">li2022seehearfeel</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{See, Hear, and Feel: Smart Sensory Fusion for Robotic Manipulation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li*, Hao and Zhang*, Yizhi and Zhu, Junzhe and Wang, Shaoxiong and Lee, Michelle A. and Xu, Huazhe and Adelson, Edward and Fei-Fei, Li and Gao†, Ruohan and Wu†, Jiajun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Robot Learning (CoRL)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/objectfolderV2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/objectfolderV2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/objectfolderV2-1400.webp"></source> <img src="/assets/img/publication_preview/objectfolderV2.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="objectfolderV2.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gao2022ObjectFolderV2" class="col-sm-8"> <div class="title">ObjectFolder 2.0: A Multisensory Object Dataset for Sim2Real Transfer</div> <div class="author"> <em>Ruohan Gao*</em>, <a href="https://si-lynnn.github.io/" rel="external nofollow noopener" target="_blank">Zilin Si*</a>, <a href="https://yuyuchang.github.io/" rel="external nofollow noopener" target="_blank">Yen-Yu Chang*</a>, <a href="https://samuelpclarke.com/" rel="external nofollow noopener" target="_blank">Samuel Clarke</a>, <a href="http://web.stanford.edu/~bohg/" rel="external nofollow noopener" target="_blank">Jeannette Bohg</a>, <a href="https://profiles.stanford.edu/fei-fei-li" rel="external nofollow noopener" target="_blank">Li Fei-Fei</a>, <a href="http://robotouch.ri.cmu.edu/yuanwz/" rel="external nofollow noopener" target="_blank">Wenzhen Yuan</a>, and <a href="https://jiajunwu.com/" rel="external nofollow noopener" target="_blank">Jiajun Wu</a> </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2204.02389.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/supps/ObjectFolderV2_Supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://github.com/rhgao/ObjectFolder" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Dataset</a> <a href="https://ai.stanford.edu/\~rhgao/objectfolder2.0/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2022ObjectFolderV2</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ObjectFolder 2.0: A Multisensory Object Dataset for Sim2Real Transfer}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao*, Ruohan and Si*, Zilin and Chang*, Yen-Yu and Clarke, Samuel and Bohg, Jeannette and Fei-Fei, Li and Yuan, Wenzhen and Wu, Jiajun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/visual_acoustic_matching_cvpr2022-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/visual_acoustic_matching_cvpr2022-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/visual_acoustic_matching_cvpr2022-1400.webp"></source> <img src="/assets/img/publication_preview/visual_acoustic_matching_cvpr2022.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="visual_acoustic_matching_cvpr2022.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="chen2022visual" class="col-sm-8"> <div class="title">Visual Acoustic Matching</div> <div class="author"> <a href="https://changan.io/" rel="external nofollow noopener" target="_blank">Changan Chen</a>, <em>Ruohan Gao</em>, <a href="https://scholar.google.com/citations?hl=en&amp;user=S_S6SZAAAAAJ" rel="external nofollow noopener" target="_blank">Paul Calamia</a>, and <a href="https://www.cs.utexas.edu/~grauman/" rel="external nofollow noopener" target="_blank">Kristen Grauman</a> </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <strong style="border-width:2px;color:red;padding:0.1em">Oral Presentation</strong> <br> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2202.06875.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/facebookresearch/visual-acoustic-matching" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://vision.cs.utexas.edu/projects/visual-acoustic-matching/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> <a href="https://www.engadget.com/metas-latest-auditory-ai-promise-a-more-immersive-ar-vr-experience-130029625.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Media Coverage</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">chen2022visual</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Visual Acoustic Matching}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chen, Changan and Gao, Ruohan and Calamia, Paul and Grauman, Kristen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/bmvc2021-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/bmvc2021-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/bmvc2021-1400.webp"></source> <img src="/assets/img/publication_preview/bmvc2021.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="bmvc2021.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="garg2021geometry" class="col-sm-8"> <div class="title">Geometry-Aware Multi-Task Learning for Binaural Audio Generation from Video</div> <div class="author"> <a href="https://bigharshrag.github.io/" rel="external nofollow noopener" target="_blank">Rishabh Garg</a>, <em>Ruohan Gao</em>, and <a href="https://www.cs.utexas.edu/~grauman/" rel="external nofollow noopener" target="_blank">Kristen Grauman</a> </div> <div class="periodical"> <em>British Machine Vision Conference (BMVC)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <strong style="border-width:2px;color:red;padding:0.1em">Best Paper Award Runner-Up</strong> <br> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2111.10882.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/supps/bmvc2021_supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://vision.cs.utexas.edu/projects/geometry-aware-binaural/#dataset" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Dataset</a> <a href="https://vision.cs.utexas.edu/projects/geometry-aware-binaural/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">garg2021geometry</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Geometry-Aware Multi-Task Learning for Binaural Audio Generation from Video}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Garg, Rishabh and Gao, Ruohan and Grauman, Kristen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{British Machine Vision Conference (BMVC)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/thesis_teaser-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/thesis_teaser-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/thesis_teaser-1400.webp"></source> <img src="/assets/img/publication_preview/thesis_teaser.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="thesis_teaser.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gao2021dissertation" class="col-sm-8"> <div class="title">Look and Listen: From Semantic to Spatial Audio-Visual Perception</div> <div class="author"> <em>Ruohan Gao</em> </div> <div class="periodical"> <em>Ph.D. Dissertation</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <strong style="border-width:2px;color:red;padding:0.1em">Michael H. Granof Award, UT Austin’s Top 1 Doctoral Dissertation</strong> <br> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/papers/Ruohan_Gao_dissertation.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://gradschool.utexas.edu/news/graduate-school-announces-2021-award-winners" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Media Coverage</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2021dissertation</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Look and Listen: From Semantic to Spatial Audio-Visual Perception}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao, Ruohan}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Ph.D. Dissertation}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/2.5D_visual_sound_cvpr2019-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/2.5D_visual_sound_cvpr2019-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/2.5D_visual_sound_cvpr2019-1400.webp"></source> <img src="/assets/img/publication_preview/2.5D_visual_sound_cvpr2019.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="2.5D_visual_sound_cvpr2019.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gao2019visual-sound" class="col-sm-8"> <div class="title">2.5D Visual Sound</div> <div class="author"> <em>Ruohan Gao</em>, and <a href="https://www.cs.utexas.edu/~grauman/" rel="external nofollow noopener" target="_blank">Kristen Grauman</a> </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> <strong style="border-width:2px;color:red;padding:0.1em">Best Paper Award Finalist</strong> <br> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/papers/gao-cvpr2019.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/supps/gao-cvpr2019-supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://github.com/facebookresearch/2.5D-Visual-Sound" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://github.com/facebookresearch/FAIR-Play" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Dataset</a> <a href="https://www.youtube.com/watch?v=JwaBi_2JFeU&amp;t=2805s" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://vision.cs.utexas.edu/projects/2.5D_visual_sound/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> <a href="https://www.technologyreview.com/2018/12/26/138088/deep-learning-turns-mono-recordings-into-immersive-sound/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Media Coverage</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2019visual-sound</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{2.5D Visual Sound}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao, Ruohan and Grauman, Kristen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/audioobjects_eccv2018-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/audioobjects_eccv2018-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/audioobjects_eccv2018-1400.webp"></source> <img src="/assets/img/publication_preview/audioobjects_eccv2018.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="audioobjects_eccv2018.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gao2018object-sounds" class="col-sm-8"> <div class="title">Learning to Separate Object Sounds by Watching Unlabeled Video</div> <div class="author"> <em>Ruohan Gao</em>, <a href="https://www.rogerioferis.org/" rel="external nofollow noopener" target="_blank">Rogerio Feris</a>, and <a href="https://www.cs.utexas.edu/~grauman/" rel="external nofollow noopener" target="_blank">Kristen Grauman</a> </div> <div class="periodical"> <em>European Conference on Computer Vision (ECCV)</em>, 2018 </div> <div class="periodical"> </div> <div class="links"> <strong style="border-width:2px;color:red;padding:0.1em">Oral Presentation</strong> <br> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/papers/sound-sep-eccv2018.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/supps/sound-sep-eccv2018_supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://github.com/rhgao/Deep-MIML-Network" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://www.youtube.com/watch?v=SDKEUdp6edc" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="/assets/posters/gao-eccv2018-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://vision.cs.utexas.edu/projects/separating_object_sounds/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2018object-sounds</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning to Separate Object Sounds by Watching Unlabeled Video}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao, Ruohan and Feris, Rogerio and Grauman, Kristen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{European Conference on Computer Vision (ECCV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Ruohan Gao. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-RK2FNYXJ3D"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RK2FNYXJ3D");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>
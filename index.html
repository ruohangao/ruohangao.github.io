<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Ruohan Gao</title> <meta name="author" content="Ruohan Gao"> <meta name="description" content="Homepage for Ruohan Gao "> <meta name="keywords" content="Ruohan Gao, Homepage, Computer Vision, Machine Learning, Artificial Intelligence, AI"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/umd.png"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ruohangao.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?6185d15ea1982787ad7f435576553d64"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%72%68%67%61%6F@%75%6D%64.%65%64%75" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=i02oEgMAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/rhgao" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://twitter.com/RuohanGao1" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">Home<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching</a> </li> <li class="nav-item"> <a class="nav-link" href="/assets/Ruohan_Gao_CV.pdf">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <article> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Ruohan-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Ruohan-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Ruohan-1400.webp"></source> <img src="/assets/img/Ruohan.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="Ruohan.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-9"> <h1 class="post-title"> Ruohan Gao </h1> <p class="desc"><b>Assistant Professor</b> <br> <a href="https://www.cs.umd.edu/" rel="external nofollow noopener" target="_blank">Department of Computer Science</a>, <a href="https://umd.edu/" rel="external nofollow noopener" target="_blank">University of Maryland, College Park</a> <br><br> Office: IRB-4248<br> Email: rhgao[AT]umd.edu </p> </div> </div> <div class="clearfix"> <p>I am an assistant professor in the <a href="https://www.cs.umd.edu/" rel="external nofollow noopener" target="_blank">Department of Computer Science</a> at <a href="https://umd.edu/" rel="external nofollow noopener" target="_blank">University of Maryland, College Park</a>, where I lead the UMD Multisensory Machine Intelligence Lab. I am also affiliated with the <a href="https://www.umiacs.umd.edu/" rel="external nofollow noopener" target="_blank">University of Maryland Institute for Advanced Computer Studies (UMIACS)</a>, <a href="https://robotics.umd.edu/" rel="external nofollow noopener" target="_blank">Maryland Robotics Center (MRC)</a>, and <a href="https://aim.umd.edu/" rel="external nofollow noopener" target="_blank">Artificial Intelligence Interdisciplinary Institute at Maryland (AIM)</a>.</p> <p>I received my Ph.D. in Computer Science from <a href="http://www.utexas.edu/" rel="external nofollow noopener" target="_blank">The University of Texas at Austin</a> advised by <a href="http://www.cs.utexas.edu/~grauman" rel="external nofollow noopener" target="_blank">Kristen Grauman</a>, and then spent two years as a PostDoc at <a href="https://svl.stanford.edu/" rel="external nofollow noopener" target="_blank">Stanford Vision and Learning Lab</a> working with <a href="https://profiles.stanford.edu/fei-fei-li" rel="external nofollow noopener" target="_blank">Fei-Fei Li</a>, <a href="https://jiajunwu.com/" rel="external nofollow noopener" target="_blank">Jiajun Wu</a>, and <a href="https://profiles.stanford.edu/silvio-savarese" rel="external nofollow noopener" target="_blank">Silvio Savarese</a>.</p> <p>My research primarily focuses on computer vision and machine learning with a particular emphasis on multisensory machine intelligence involving sight, sound, and touch. The overarching goal of my research is to empower machines to emulate and enhance human capabilities in seeing, hearing, and feeling, ultimately enabling them to comprehensively perceive, understand, and interact with the multisensory world.</p> <p><strong><em><span style="color:red">Prospective Students:</span></em></strong> I am always seeking self-motivated students to join my group. If you are interested, <a href="./prospective_students">here is some more information</a>.</p> </div> <h2>Selected Publications <font size="5"><a href="/publications/">[full list]</a></font> </h2> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/avdar_2025-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/avdar_2025-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/avdar_2025-1400.webp"></source> <img src="/assets/img/publication_preview/avdar_2025.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="avdar_2025.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="jin2025avdar" class="col-sm-8"> <div class="title">Differentiable Room Acoustic Rendering with Multi-View Vision Priors</div> <div class="author"> <a href="https://humathe.github.io/" rel="external nofollow noopener" target="_blank">Derong Jin</a>, and <em>Ruohan Gao</em> </div> <div class="periodical"> <em>arXiv</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2504.21847" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.youtube.com/watch?v=thtuB1kRwPU" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://humathe.github.io/avdar/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">jin2025avdar</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Differentiable Room Acoustic Rendering with Multi-View Vision Priors}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jin, Derong and Gao, Ruohan}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/HAAE_cvpr2025-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/HAAE_cvpr2025-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/HAAE_cvpr2025-1400.webp"></source> <img src="/assets/img/publication_preview/HAAE_cvpr2025.jpeg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="HAAE_cvpr2025.jpeg" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="liu2025haae" class="col-sm-8"> <div class="title">Hearing Anywhere in Any Environment</div> <div class="author"> <a href="https://dragonliu1995.github.io/" rel="external nofollow noopener" target="_blank">Xiulong Liu</a>, <a href="https://anuragkr90.github.io/" rel="external nofollow noopener" target="_blank">Anurag Kumar</a>, <a href="https://scholar.google.com/citations?hl=en&amp;user=S_S6SZAAAAAJ" rel="external nofollow noopener" target="_blank">Paul Calamia</a>, Sebastià V. Amengual Garí, Calvin Murdock, <a href="https://www.ishwarya.me/" rel="external nofollow noopener" target="_blank">Ishwarya Ananthabhotla</a>, Philip Robinson, <a href="https://faculty.washington.edu/shlizee/" rel="external nofollow noopener" target="_blank">Eli Shlizerman</a>, <a href="https://www.vamsiithapu.com/" rel="external nofollow noopener" target="_blank">Vamsi Krishna Ithapu</a>, and <em>Ruohan Gao</em> </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2504.10746" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">liu2025haae</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Hearing Anywhere in Any Environment}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Xiulong and Kumar, Anurag and Calamia, Paul and Garí, Sebastià V. Amengual and Murdock, Calvin and Ananthabhotla, Ishwarya and Robinson, Philip and Shlizerman, Eli and Ithapu, Vamsi Krishna and Gao, Ruohan}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/visal_cvpr_2025-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/visal_cvpr_2025-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/visal_cvpr_2025-1400.webp"></source> <img src="/assets/img/publication_preview/visal_cvpr_2025.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="visal_cvpr_2025.jpg" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="huang2025visal" class="col-sm-8"> <div class="title">Learning to Highlight Audio by Watching Movies</div> <div class="author"> <a href="https://wikichao.github.io/" rel="external nofollow noopener" target="_blank">Chao Huang</a>, <em>Ruohan Gao</em>, J. M. F. Tsang, Jan Kurcius, Cagdas Bilen, <a href="https://www.cs.rochester.edu/~cxu22/" rel="external nofollow noopener" target="_blank">Chenliang Xu</a>, <a href="https://anuragkr90.github.io/" rel="external nofollow noopener" target="_blank">Anurag Kumar</a>, and <a href="https://sanjeelparekh.github.io/" rel="external nofollow noopener" target="_blank">Sanjeel Parekh</a> </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2505.12154" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://wikichao.github.io/VisAH/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">huang2025visal</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning to Highlight Audio by Watching Movies}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Huang, Chao and Gao, Ruohan and Tsang, J. M. F. and Kurcius, Jan and Bilen, Cagdas and Xu, Chenliang and Kumar, Anurag and Parekh, Sanjeel}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/hearing_anything_anywhere_cvpr2024-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/hearing_anything_anywhere_cvpr2024-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/hearing_anything_anywhere_cvpr2024-1400.webp"></source> <img src="/assets/img/publication_preview/hearing_anything_anywhere_cvpr2024.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="hearing_anything_anywhere_cvpr2024.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2024haa" class="col-sm-8"> <div class="title">Hearing Anything Anywhere</div> <div class="author"> <a href="https://masonlwang.com/" rel="external nofollow noopener" target="_blank">Mason L. Wang*</a>, Ryosuke Sawata*, <a href="https://samuelpclarke.com/" rel="external nofollow noopener" target="_blank">Samuel Clarke</a>, <em>Ruohan Gao</em>, <a href="https://elliottwu.com/" rel="external nofollow noopener" target="_blank">Shangzhe Wu</a>, and <a href="https://jiajunwu.com/" rel="external nofollow noopener" target="_blank">Jiajun Wu</a> </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2406.07532" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/maswang32/hearinganythinganywhere/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://zenodo.org/records/11195833" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Dataset</a> <a href="https://www.youtube.com/watch?v=SHMy72fzU4Y" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://masonlwang.com/hearinganythinganywhere/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wang2024haa</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Hearing Anything Anywhere}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang*, Mason L. and Sawata*, Ryosuke and Clarke, Samuel and Gao, Ruohan and Wu, Shangzhe and Wu, Jiajun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/of_benchmark_cvpr2023-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/of_benchmark_cvpr2023-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/of_benchmark_cvpr2023-1400.webp"></source> <img src="/assets/img/publication_preview/of_benchmark_cvpr2023.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="of_benchmark_cvpr2023.jpg" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gao2023ObjectFolderBM" class="col-sm-8"> <div class="title">The ObjectFolder Benchmark: Multisensory Object-Centric Learning with Neural and Real Objects</div> <div class="author"> <em>Ruohan Gao*</em>, <a href="https://dou-yiming.github.io/" rel="external nofollow noopener" target="_blank">Yiming Dou*</a>, <a href="https://haolirobo.github.io/" rel="external nofollow noopener" target="_blank">Hao Li*</a>, <a href="https://tanmay-agarwal.com/" rel="external nofollow noopener" target="_blank">Tanmay Agarwal</a>, <a href="http://web.stanford.edu/~bohg/" rel="external nofollow noopener" target="_blank">Jeannette Bohg</a>, <a href="https://yunzhuli.github.io/" rel="external nofollow noopener" target="_blank">Yunzhu Li</a>, <a href="https://profiles.stanford.edu/fei-fei-li" rel="external nofollow noopener" target="_blank">Li Fei-Fei</a>, and <a href="https://jiajunwu.com/" rel="external nofollow noopener" target="_blank">Jiajun Wu</a> </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2306.00956.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/objectfolder" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://www.youtube.com/watch?v=VhXDempUYgE" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://objectfolder.stanford.edu/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> <a href="https://www.objectfolder.org/swan_vis/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Interactive Demo</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2023ObjectFolderBM</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{The ObjectFolder Benchmark: Multisensory Object-Centric Learning with Neural and Real Objects}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao*, Ruohan and Dou*, Yiming and Li*, Hao and Agarwal, Tanmay and Bohg, Jeannette and Li, Yunzhu and Fei-Fei, Li and Wu, Jiajun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/realimpact_cvpr2023-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/realimpact_cvpr2023-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/realimpact_cvpr2023-1400.webp"></source> <img src="/assets/img/publication_preview/realimpact_cvpr2023.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="realimpact_cvpr2023.jpg" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="clarke2023realimpact" class="col-sm-8"> <div class="title">RealImpact: A Dataset of Impact Sound Fields for Real Objects</div> <div class="author"> <a href="https://samuelpclarke.com/" rel="external nofollow noopener" target="_blank">Samuel Clarke</a>, <em>Ruohan Gao</em>, <a href="https://masonlwang.com/" rel="external nofollow noopener" target="_blank">Mason Wang</a>, <a href="https://ccrma.stanford.edu/~mrau/" rel="external nofollow noopener" target="_blank">Mark Rau</a>, Julia Xu, <a href="https://ccrma.stanford.edu/~mrau/" rel="external nofollow noopener" target="_blank">Mark Rau</a>, <a href="http://juiwang.com/" rel="external nofollow noopener" target="_blank">Jui-Hsien Wang</a>, <a href="http://graphics.stanford.edu/~djames/" rel="external nofollow noopener" target="_blank">Doug James</a>, and <a href="https://jiajunwu.com/" rel="external nofollow noopener" target="_blank">Jiajun Wu</a> </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <strong style="border-width:2px;color:red;padding:0.1em">Highlight Paper</strong> <br> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2306.09944.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/supps/RealImpact_supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://github.com/samuel-clarke/RealImpact" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://www.youtube.com/watch?v=q_meSUnzZo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="hhttps://samuelpclarke.com/realimpact/" class="btn btn-sm z-depth-0" role="button">Project Page</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">clarke2023realimpact</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{RealImpact: A Dataset of Impact Sound Fields for Real Objects}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Clarke, Samuel and Gao, Ruohan and Wang, Mason and Rau, Mark and Xu, Julia and Rau, Mark and Wang, Jui-Hsien and James, Doug and Wu, Jiajun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/see_hear_feel_corl2022-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/see_hear_feel_corl2022-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/see_hear_feel_corl2022-1400.webp"></source> <img src="/assets/img/publication_preview/see_hear_feel_corl2022.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="see_hear_feel_corl2022.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="li2022seehearfeel" class="col-sm-8"> <div class="title">See, Hear, and Feel: Smart Sensory Fusion for Robotic Manipulation</div> <div class="author"> <a href="https://haolirobo.github.io/" rel="external nofollow noopener" target="_blank">Hao Li*</a>, Yizhi Zhang*, Junzhe Zhu, <a href="http://shaoxiongwang.com/" rel="external nofollow noopener" target="_blank">Shaoxiong Wang</a>, <a href="https://scholar.google.com/citations?hl=en&amp;user=2Dmb3XYAAAAJ" rel="external nofollow noopener" target="_blank">Michelle A. Lee</a>, <a href="http://hxu.rocks/" rel="external nofollow noopener" target="_blank">Huazhe Xu</a>, <a href="http://persci.mit.edu/people/adelson" rel="external nofollow noopener" target="_blank">Edward Adelson</a>, <a href="https://profiles.stanford.edu/fei-fei-li" rel="external nofollow noopener" target="_blank">Li Fei-Fei</a>, <em>Ruohan Gao†</em>, and <a href="https://jiajunwu.com/" rel="external nofollow noopener" target="_blank">Jiajun Wu†</a> </div> <div class="periodical"> <em>Conference on Robot Learning (CoRL)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2212.03858.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/supps/seel_hear_feel_supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://www.youtube.com/watch?v=sRdx3sa6ryk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://ai.stanford.edu/\~rhgao/see_hear_feel/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">li2022seehearfeel</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{See, Hear, and Feel: Smart Sensory Fusion for Robotic Manipulation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li*, Hao and Zhang*, Yizhi and Zhu, Junzhe and Wang, Shaoxiong and Lee, Michelle A. and Xu, Huazhe and Adelson, Edward and Fei-Fei, Li and Gao†, Ruohan and Wu†, Jiajun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Robot Learning (CoRL)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/objectfolderV2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/objectfolderV2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/objectfolderV2-1400.webp"></source> <img src="/assets/img/publication_preview/objectfolderV2.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="objectfolderV2.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gao2022ObjectFolderV2" class="col-sm-8"> <div class="title">ObjectFolder 2.0: A Multisensory Object Dataset for Sim2Real Transfer</div> <div class="author"> <em>Ruohan Gao*</em>, <a href="https://si-lynnn.github.io/" rel="external nofollow noopener" target="_blank">Zilin Si*</a>, <a href="https://yuyuchang.github.io/" rel="external nofollow noopener" target="_blank">Yen-Yu Chang*</a>, <a href="https://samuelpclarke.com/" rel="external nofollow noopener" target="_blank">Samuel Clarke</a>, <a href="http://web.stanford.edu/~bohg/" rel="external nofollow noopener" target="_blank">Jeannette Bohg</a>, <a href="https://profiles.stanford.edu/fei-fei-li" rel="external nofollow noopener" target="_blank">Li Fei-Fei</a>, <a href="http://robotouch.ri.cmu.edu/yuanwz/" rel="external nofollow noopener" target="_blank">Wenzhen Yuan</a>, and <a href="https://jiajunwu.com/" rel="external nofollow noopener" target="_blank">Jiajun Wu</a> </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2204.02389.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/supps/ObjectFolderV2_Supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://github.com/rhgao/ObjectFolder" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Dataset</a> <a href="https://ai.stanford.edu/\~rhgao/objectfolder2.0/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2022ObjectFolderV2</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ObjectFolder 2.0: A Multisensory Object Dataset for Sim2Real Transfer}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao*, Ruohan and Si*, Zilin and Chang*, Yen-Yu and Clarke, Samuel and Bohg, Jeannette and Fei-Fei, Li and Yuan, Wenzhen and Wu, Jiajun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/visual_acoustic_matching_cvpr2022-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/visual_acoustic_matching_cvpr2022-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/visual_acoustic_matching_cvpr2022-1400.webp"></source> <img src="/assets/img/publication_preview/visual_acoustic_matching_cvpr2022.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="visual_acoustic_matching_cvpr2022.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="chen2022visual" class="col-sm-8"> <div class="title">Visual Acoustic Matching</div> <div class="author"> <a href="https://changan.io/" rel="external nofollow noopener" target="_blank">Changan Chen</a>, <em>Ruohan Gao</em>, <a href="https://scholar.google.com/citations?hl=en&amp;user=S_S6SZAAAAAJ" rel="external nofollow noopener" target="_blank">Paul Calamia</a>, and <a href="https://www.cs.utexas.edu/~grauman/" rel="external nofollow noopener" target="_blank">Kristen Grauman</a> </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <strong style="border-width:2px;color:red;padding:0.1em">Oral Presentation</strong> <br> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2202.06875.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/facebookresearch/visual-acoustic-matching" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://vision.cs.utexas.edu/projects/visual-acoustic-matching/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> <a href="https://www.engadget.com/metas-latest-auditory-ai-promise-a-more-immersive-ar-vr-experience-130029625.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Media Coverage</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">chen2022visual</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Visual Acoustic Matching}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chen, Changan and Gao, Ruohan and Calamia, Paul and Grauman, Kristen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/bmvc2021-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/bmvc2021-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/bmvc2021-1400.webp"></source> <img src="/assets/img/publication_preview/bmvc2021.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="bmvc2021.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="garg2021geometry" class="col-sm-8"> <div class="title">Geometry-Aware Multi-Task Learning for Binaural Audio Generation from Video</div> <div class="author"> <a href="https://bigharshrag.github.io/" rel="external nofollow noopener" target="_blank">Rishabh Garg</a>, <em>Ruohan Gao</em>, and <a href="https://www.cs.utexas.edu/~grauman/" rel="external nofollow noopener" target="_blank">Kristen Grauman</a> </div> <div class="periodical"> <em>British Machine Vision Conference (BMVC)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <strong style="border-width:2px;color:red;padding:0.1em">Best Paper Award Runner-Up</strong> <br> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2111.10882.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/supps/bmvc2021_supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://vision.cs.utexas.edu/projects/geometry-aware-binaural/#dataset" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Dataset</a> <a href="https://vision.cs.utexas.edu/projects/geometry-aware-binaural/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">garg2021geometry</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Geometry-Aware Multi-Task Learning for Binaural Audio Generation from Video}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Garg, Rishabh and Gao, Ruohan and Grauman, Kristen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{British Machine Vision Conference (BMVC)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/thesis_teaser-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/thesis_teaser-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/thesis_teaser-1400.webp"></source> <img src="/assets/img/publication_preview/thesis_teaser.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="thesis_teaser.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gao2021dissertation" class="col-sm-8"> <div class="title">Look and Listen: From Semantic to Spatial Audio-Visual Perception</div> <div class="author"> <em>Ruohan Gao</em> </div> <div class="periodical"> <em>Ph.D. Dissertation</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <strong style="border-width:2px;color:red;padding:0.1em">Michael H. Granof Award, UT Austin’s Top 1 Doctoral Dissertation</strong> <br> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/papers/Ruohan_Gao_dissertation.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://gradschool.utexas.edu/news/graduate-school-announces-2021-award-winners" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Media Coverage</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2021dissertation</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Look and Listen: From Semantic to Spatial Audio-Visual Perception}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao, Ruohan}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Ph.D. Dissertation}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/2.5D_visual_sound_cvpr2019-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/2.5D_visual_sound_cvpr2019-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/2.5D_visual_sound_cvpr2019-1400.webp"></source> <img src="/assets/img/publication_preview/2.5D_visual_sound_cvpr2019.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="2.5D_visual_sound_cvpr2019.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gao2019visual-sound" class="col-sm-8"> <div class="title">2.5D Visual Sound</div> <div class="author"> <em>Ruohan Gao</em>, and <a href="https://www.cs.utexas.edu/~grauman/" rel="external nofollow noopener" target="_blank">Kristen Grauman</a> </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> <strong style="border-width:2px;color:red;padding:0.1em">Best Paper Award Finalist</strong> <br> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/papers/gao-cvpr2019.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/supps/gao-cvpr2019-supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://github.com/facebookresearch/2.5D-Visual-Sound" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://github.com/facebookresearch/FAIR-Play" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Dataset</a> <a href="https://www.youtube.com/watch?v=JwaBi_2JFeU&amp;t=2805s" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://vision.cs.utexas.edu/projects/2.5D_visual_sound/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> <a href="https://www.technologyreview.com/2018/12/26/138088/deep-learning-turns-mono-recordings-into-immersive-sound/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Media Coverage</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2019visual-sound</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{2.5D Visual Sound}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao, Ruohan and Grauman, Kristen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/audioobjects_eccv2018-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/audioobjects_eccv2018-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/audioobjects_eccv2018-1400.webp"></source> <img src="/assets/img/publication_preview/audioobjects_eccv2018.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="audioobjects_eccv2018.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gao2018object-sounds" class="col-sm-8"> <div class="title">Learning to Separate Object Sounds by Watching Unlabeled Video</div> <div class="author"> <em>Ruohan Gao</em>, <a href="https://www.rogerioferis.org/" rel="external nofollow noopener" target="_blank">Rogerio Feris</a>, and <a href="https://www.cs.utexas.edu/~grauman/" rel="external nofollow noopener" target="_blank">Kristen Grauman</a> </div> <div class="periodical"> <em>European Conference on Computer Vision (ECCV)</em>, 2018 </div> <div class="periodical"> </div> <div class="links"> <strong style="border-width:2px;color:red;padding:0.1em">Oral Presentation</strong> <br> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/papers/sound-sep-eccv2018.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/supps/sound-sep-eccv2018_supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://github.com/rhgao/Deep-MIML-Network" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://www.youtube.com/watch?v=SDKEUdp6edc" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="/assets/posters/gao-eccv2018-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://vision.cs.utexas.edu/projects/separating_object_sounds/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2018object-sounds</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning to Separate Object Sounds by Watching Unlabeled Video}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao, Ruohan and Feris, Rogerio and Grauman, Kristen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{European Conference on Computer Vision (ECCV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Ruohan Gao. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?142c2d16ef599a956d8aaea0e21679dc" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-RK2FNYXJ3D"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RK2FNYXJ3D");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>